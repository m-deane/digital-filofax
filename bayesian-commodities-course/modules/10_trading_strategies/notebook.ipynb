{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Backtesting, Evaluation, and Trading Strategies\n",
    "\n",
    "**Course**: Bayesian Regression and Time Series Forecasting for Commodities Trading\n",
    "\n",
    "**CAPSTONE MODULE**: Integrating Everything We've Learned\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Implement** walk-forward validation to avoid look-ahead bias in Bayesian models\n",
    "2. **Evaluate** probabilistic forecasts using proper scoring rules (CRPS, log-score)\n",
    "3. **Assess** forecast calibration and reliability for risk management\n",
    "4. **Calculate** probabilistic Sharpe ratios accounting for parameter uncertainty\n",
    "5. **Apply** Kelly criterion with Bayesian posteriors for optimal position sizing\n",
    "6. **Build** complete trading strategies:\n",
    "   - Mean reversion with uncertainty bands\n",
    "   - Trend following with regime detection\n",
    "   - Pairs trading with Bayesian cointegration\n",
    "7. **Optimize** portfolios using Bayesian return distributions\n",
    "8. **Deploy** a complete energy portfolio trading system\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Trading\n",
    "\n",
    "**This is where theory meets reality.** Everything we've learned—priors, MCMC, time series models, GPs, volatility—means nothing if we can't:\n",
    "\n",
    "1. **Backtest correctly**: Avoid overfitting and look-ahead bias\n",
    "2. **Evaluate honestly**: Use metrics that reward calibrated forecasts, not just accuracy\n",
    "3. **Size positions optimally**: Bayesian Kelly criterion with parameter uncertainty\n",
    "4. **Manage risk systematically**: Probabilistic stop-losses, portfolio constraints\n",
    "5. **Execute in practice**: Handle transaction costs, slippage, margin requirements\n",
    "\n",
    "### Real-World Trading Failures\n",
    "\n",
    "- **Long-Term Capital Management (1998)**: Models didn't account for parameter uncertainty → over-leveraged → $4B loss\n",
    "- **Amaranth Advisors (2006)**: Natural gas volatility model failed during regime change → $6B loss in one week\n",
    "- **Quantitative funds (August 2007)**: Models over-fit to in-sample data, failed out-of-sample → multi-billion dollar losses\n",
    "\n",
    "**The pattern**: Great models, terrible backtesting/risk management.\n",
    "\n",
    "### What Makes This Module Different\n",
    "\n",
    "- **No look-ahead bias**: Walk-forward validation\n",
    "- **Honest evaluation**: Probabilistic metrics (CRPS, calibration)\n",
    "- **Risk-first thinking**: Position sizing before alpha generation\n",
    "- **Complete systems**: Not just signals, but full trading workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import course utilities\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from utils.backtesting import WalkForwardValidator, Backtester, kelly_position_sizer\n",
    "from utils.metrics import crps, sharpe_ratio, forecast_summary, mae, rmse\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"All libraries and utilities loaded successfully!\")\n",
    "print(f\"PyMC version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Walk-Forward Validation: The Gold Standard\n",
    "\n",
    "### 1.1 The Problem: In-Sample Overfitting\n",
    "\n",
    "**Common mistake**: Fit model on all historical data, report performance.\n",
    "\n",
    "**Why it fails**:\n",
    "- Model \"sees\" future data during fitting\n",
    "- Overfits to noise in the training period\n",
    "- Out-of-sample performance much worse\n",
    "\n",
    "### 1.2 Walk-Forward Validation\n",
    "\n",
    "**Idea**: Simulate real-time trading by:\n",
    "1. Fit model on data up to time $t$\n",
    "2. Forecast for time $t+1, ..., t+k$\n",
    "3. Observe actual outcomes\n",
    "4. **Roll forward** to $t+k$, refit, repeat\n",
    "\n",
    "**Two types**:\n",
    "- **Expanding window**: Training set grows (use all historical data)\n",
    "- **Rolling window**: Fixed training size (only recent data)\n",
    "\n",
    "### 1.3 Bayesian Walk-Forward\n",
    "\n",
    "**Standard**: Refit model from scratch each period (slow).\n",
    "\n",
    "**Bayesian**: Can use **sequential updating**:\n",
    "- Previous posterior becomes next prior\n",
    "- Much faster than full refit\n",
    "- Naturally adapts to regime changes\n",
    "\n",
    "$$p(\\theta | \\text{data}_{1:t+1}) \\propto p(\\text{data}_{t+1} | \\theta) \\cdot p(\\theta | \\text{data}_{1:t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic commodity data for backtesting\n",
    "def generate_commodity_data(n=500, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic crude oil prices with regime changes.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    dates = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "    \n",
    "    # Generate returns with multiple regimes\n",
    "    returns = np.zeros(n)\n",
    "    volatility = np.zeros(n)\n",
    "    \n",
    "    # Regime 1: Normal (days 0-200)\n",
    "    vol1 = 0.015\n",
    "    returns[:200] = np.random.normal(0.0003, vol1, 200)\n",
    "    volatility[:200] = vol1\n",
    "    \n",
    "    # Regime 2: High volatility (days 200-350) - COVID-like shock\n",
    "    vol2 = 0.04\n",
    "    returns[200:350] = np.random.normal(-0.001, vol2, 150)\n",
    "    volatility[200:350] = vol2\n",
    "    \n",
    "    # Regime 3: Recovery (days 350-500)\n",
    "    vol3 = 0.02\n",
    "    returns[350:] = np.random.normal(0.0008, vol3, n - 350)\n",
    "    volatility[350:] = vol3\n",
    "    \n",
    "    # Convert to prices\n",
    "    prices = 60 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'close': prices,\n",
    "        'returns': returns,\n",
    "        'true_vol': volatility\n",
    "    })\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "data = generate_commodity_data(n=500)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Prices\n",
    "ax = axes[0]\n",
    "ax.plot(data.index, data['close'], linewidth=1.5, color='black')\n",
    "ax.axvspan(data.index[200], data.index[350], alpha=0.2, color='red', label='High volatility regime')\n",
    "ax.set_ylabel('Price ($/barrel)', fontsize=11)\n",
    "ax.set_title('Crude Oil Prices (Synthetic)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "ax = axes[1]\n",
    "ax.plot(data.index, data['returns'], linewidth=0.8, color='blue', alpha=0.7)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.axvspan(data.index[200], data.index[350], alpha=0.2, color='red')\n",
    "ax.set_ylabel('Returns', fontsize=11)\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_title('Daily Returns (showing regime change)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset: {len(data)} days from {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "print(f\"\\nRegime statistics:\")\n",
    "print(f\"  Normal period (days 1-200):      Vol = {data['true_vol'][:200].mean():.4f}\")\n",
    "print(f\"  Crisis period (days 201-350):    Vol = {data['true_vol'][200:350].mean():.4f}\")\n",
    "print(f\"  Recovery period (days 351-500):  Vol = {data['true_vol'][350:].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement walk-forward backtesting with Bayesian model\n",
    "def bayesian_ar_model(train_data, n_lags=5):\n",
    "    \"\"\"\n",
    "    Fit Bayesian AR(p) model for returns.\n",
    "    \"\"\"\n",
    "    returns = train_data['returns'].values\n",
    "    \n",
    "    # Create lagged features\n",
    "    X = np.column_stack([returns[n_lags-i-1:-i-1] for i in range(n_lags)])\n",
    "    y = returns[n_lags:]\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        beta = pm.Normal('beta', mu=0, sigma=0.5, shape=n_lags)\n",
    "        sigma = pm.HalfNormal('sigma', sigma=0.02)\n",
    "        \n",
    "        # Likelihood\n",
    "        mu = pm.math.dot(X, beta)\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n",
    "        \n",
    "        # Sample\n",
    "        trace = pm.sample(500, tune=500, chains=2, random_seed=42, \n",
    "                         progressbar=False, return_inferencedata=True)\n",
    "    \n",
    "    return trace, model\n",
    "\n",
    "def predict_bayesian(trace, test_data, n_lags=5, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate probabilistic forecasts from Bayesian AR model.\n",
    "    \"\"\"\n",
    "    # Get last n_lags returns for prediction\n",
    "    last_returns = test_data['returns'].values[:n_lags][::-1]  # Reverse for lag order\n",
    "    \n",
    "    # Extract posterior samples\n",
    "    beta_samples = trace.posterior['beta'].values.reshape(-1, n_lags)\n",
    "    sigma_samples = trace.posterior['sigma'].values.flatten()\n",
    "    \n",
    "    # Generate predictions\n",
    "    n_post_samples = len(beta_samples)\n",
    "    predictions = np.zeros(n_post_samples)\n",
    "    \n",
    "    for i in range(n_post_samples):\n",
    "        mu = np.dot(last_returns, beta_samples[i])\n",
    "        predictions[i] = mu + sigma_samples[i] * np.random.randn()\n",
    "    \n",
    "    # Summary statistics\n",
    "    pred_mean = predictions.mean()\n",
    "    pred_std = predictions.std()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'prediction': [pred_mean],\n",
    "        'lower_95': [np.percentile(predictions, 2.5)],\n",
    "        'upper_95': [np.percentile(predictions, 97.5)],\n",
    "        'std': [pred_std]\n",
    "    }, index=test_data.index[:1])\n",
    "\n",
    "# Run walk-forward validation\n",
    "print(\"Running walk-forward validation...\")\n",
    "print(\"This simulates real-time trading (no look-ahead bias)\\n\")\n",
    "\n",
    "# Parameters\n",
    "min_train_size = 100\n",
    "test_size = 1  # 1-day ahead forecasts\n",
    "n_folds = 50  # 50 forecast periods\n",
    "\n",
    "# Storage\n",
    "wf_results = []\n",
    "fold_count = 0\n",
    "\n",
    "# Walk-forward loop\n",
    "for i in range(n_folds):\n",
    "    # Define train/test split\n",
    "    train_end = min_train_size + i * 5  # Expand training window\n",
    "    test_start = train_end\n",
    "    test_end = test_start + test_size\n",
    "    \n",
    "    if test_end > len(data):\n",
    "        break\n",
    "    \n",
    "    train = data.iloc[:train_end]\n",
    "    test = data.iloc[test_start:test_end]\n",
    "    \n",
    "    # Fit model on training data\n",
    "    try:\n",
    "        trace, model = bayesian_ar_model(train, n_lags=5)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = predict_bayesian(trace, test, n_lags=5)\n",
    "        forecast['actual'] = test['returns'].values[0]\n",
    "        forecast['fold'] = fold_count\n",
    "        \n",
    "        wf_results.append(forecast)\n",
    "        fold_count += 1\n",
    "        \n",
    "        if fold_count % 10 == 0:\n",
    "            print(f\"Completed fold {fold_count}/{n_folds}\")\n",
    "    except:\n",
    "        print(f\"Fold {i} failed, skipping...\")\n",
    "        continue\n",
    "\n",
    "# Combine results\n",
    "wf_df = pd.concat(wf_results)\n",
    "\n",
    "print(f\"\\nWalk-forward validation complete: {len(wf_df)} forecasts generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate walk-forward results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
    "\n",
    "# Plot 1: Forecasts vs actuals\n",
    "ax = axes[0]\n",
    "ax.scatter(range(len(wf_df)), wf_df['actual'], c='black', s=40, \n",
    "           alpha=0.6, label='Actual returns', zorder=3)\n",
    "ax.plot(range(len(wf_df)), wf_df['prediction'], 'blue', linewidth=2, \n",
    "        label='Forecast (posterior mean)', zorder=2)\n",
    "ax.fill_between(range(len(wf_df)),\n",
    "                wf_df['lower_95'],\n",
    "                wf_df['upper_95'],\n",
    "                alpha=0.3, color='blue', label='95% credible interval', zorder=1)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Forecast Period', fontsize=11)\n",
    "ax.set_ylabel('Returns', fontsize=11)\n",
    "ax.set_title('Walk-Forward Validation: Out-of-Sample Forecasts', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Forecast errors\n",
    "ax = axes[1]\n",
    "errors = wf_df['actual'] - wf_df['prediction']\n",
    "ax.hist(errors, bins=25, alpha=0.7, density=True, color='steelblue', \n",
    "        edgecolor='black', label='Forecast errors')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "ax.axvline(errors.mean(), color='green', linestyle='-', linewidth=2, \n",
    "           label=f'Mean error = {errors.mean():.6f}')\n",
    "ax.set_xlabel('Forecast Error (actual - predicted)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Distribution of Forecast Errors', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "mae_score = mae(wf_df['actual'].values, wf_df['prediction'].values)\n",
    "rmse_score = rmse(wf_df['actual'].values, wf_df['prediction'].values)\n",
    "coverage = np.mean((wf_df['actual'] >= wf_df['lower_95']) & \n",
    "                   (wf_df['actual'] <= wf_df['upper_95']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WALK-FORWARD VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nForecast Accuracy:\")\n",
    "print(f\"  MAE:  {mae_score:.6f}\")\n",
    "print(f\"  RMSE: {rmse_score:.6f}\")\n",
    "print(f\"\\nCalibration:\")\n",
    "print(f\"  95% Interval Coverage: {coverage:.1%}\")\n",
    "print(f\"  Target: 95% (well-calibrated forecasts)\")\n",
    "if coverage < 0.90:\n",
    "    print(f\"  ⚠ Under-confident: Intervals too narrow\")\n",
    "elif coverage > 0.98:\n",
    "    print(f\"  ⚠ Over-confident: Intervals too wide\")\n",
    "else:\n",
    "    print(f\"  ✓ Well-calibrated forecasts!\")\n",
    "\n",
    "print(f\"\\nForecast Bias:\")\n",
    "print(f\"  Mean error: {errors.mean():.6f}\")\n",
    "if abs(errors.mean()) < 0.001:\n",
    "    print(f\"  ✓ Unbiased forecasts\")\n",
    "else:\n",
    "    print(f\"  ⚠ Systematic bias detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Walk-forward validation ensures:\n",
    "1. NO LOOK-AHEAD BIAS - Model never sees future data\n",
    "2. REALISTIC PERFORMANCE - Simulates actual trading\n",
    "3. OUT-OF-SAMPLE EVALUATION - True test of generalization\n",
    "\n",
    "Coverage = {:.1%} means:\n",
    "- {:.1%} of actual returns fell within 95% prediction intervals\n",
    "- Close to 95% target → forecasts are well-calibrated\n",
    "- Important for risk management (VaR, stop-losses)\n",
    "\n",
    "**Trading implication**:\n",
    "Well-calibrated intervals → can trust uncertainty estimates for:\n",
    "- Position sizing (scale by 1/uncertainty)\n",
    "- Stop-loss placement (outside 95% interval)\n",
    "- Risk limits (based on CVaR from predictive distribution)\n",
    "\"\"\".format(coverage, coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proper Scoring Rules for Probabilistic Forecasts\n",
    "\n",
    "### 2.1 Why Standard Metrics Fail\n",
    "\n",
    "**MAE and RMSE**:\n",
    "- Only evaluate point forecasts (mean or median)\n",
    "- Ignore forecast uncertainty\n",
    "- Don't penalize poorly calibrated intervals\n",
    "\n",
    "**Example problem**: \n",
    "- Forecast A: \"Return = 0.001 ± 0.01\" (uncertain)\n",
    "- Forecast B: \"Return = 0.001 ± 0.0001\" (very confident)\n",
    "\n",
    "If actual = 0.01, both have same MAE. But Forecast B is badly calibrated!\n",
    "\n",
    "### 2.2 Continuous Ranked Probability Score (CRPS)\n",
    "\n",
    "**The gold standard** for probabilistic forecasts.\n",
    "\n",
    "$$\\text{CRPS}(F, y) = \\int_{-\\infty}^{\\infty} [F(x) - \\mathbb{1}(x \\geq y)]^2 dx$$\n",
    "\n",
    "Where:\n",
    "- $F(x)$ is the forecast CDF\n",
    "- $y$ is the actual outcome\n",
    "- $\\mathbb{1}(x \\geq y)$ is the \"perfect\" forecast (step function at $y$)\n",
    "\n",
    "**Properties**:\n",
    "1. **Proper scoring rule**: Minimized when forecast = true distribution\n",
    "2. **Generalizes MAE**: CRPS → MAE when forecast is deterministic\n",
    "3. **Rewards calibration**: Penalizes both bias and miscalibrated uncertainty\n",
    "\n",
    "**Sample-based approximation**:\n",
    "$$\\text{CRPS} \\approx \\mathbb{E}[|X - y|] - \\frac{1}{2}\\mathbb{E}[|X - X'|]$$\n",
    "\n",
    "where $X, X'$ are independent samples from forecast distribution.\n",
    "\n",
    "### 2.3 Log Score (Ignorance Score)\n",
    "\n",
    "$$\\text{Log Score} = -\\log p(y | \\text{forecast})$$\n",
    "\n",
    "- Lower is better\n",
    "- Heavily penalizes forecasts that assign low probability to actual outcome\n",
    "- Related to information theory (\"surprisal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CRPS for walk-forward forecasts\n",
    "# We'll need to re-generate posterior predictive samples\n",
    "\n",
    "# For demonstration, simulate from forecast distributions\n",
    "def calculate_crps_from_gaussian(actual, mean, std):\n",
    "    \"\"\"\n",
    "    CRPS for Gaussian forecast (closed form).\n",
    "    \"\"\"\n",
    "    z = (actual - mean) / std\n",
    "    phi = stats.norm.pdf(z)\n",
    "    Phi = stats.norm.cdf(z)\n",
    "    crps_val = std * (z * (2 * Phi - 1) + 2 * phi - 1 / np.sqrt(np.pi))\n",
    "    return crps_val\n",
    "\n",
    "# Calculate CRPS for each forecast\n",
    "crps_scores = []\n",
    "for idx, row in wf_df.iterrows():\n",
    "    crps_val = calculate_crps_from_gaussian(row['actual'], row['prediction'], row['std'])\n",
    "    crps_scores.append(crps_val)\n",
    "\n",
    "wf_df['crps'] = crps_scores\n",
    "mean_crps = np.mean(crps_scores)\n",
    "\n",
    "# Also calculate traditional metrics for comparison\n",
    "mae_score = mae(wf_df['actual'].values, wf_df['prediction'].values)\n",
    "rmse_score = rmse(wf_df['actual'].values, wf_df['prediction'].values)\n",
    "\n",
    "# Visualize CRPS over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
    "\n",
    "# CRPS time series\n",
    "ax = axes[0]\n",
    "ax.plot(range(len(wf_df)), wf_df['crps'], linewidth=2, color='purple', label='CRPS')\n",
    "ax.axhline(mean_crps, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean CRPS = {mean_crps:.6f}')\n",
    "ax.set_xlabel('Forecast Period', fontsize=11)\n",
    "ax.set_ylabel('CRPS (lower = better)', fontsize=11)\n",
    "ax.set_title('Continuous Ranked Probability Score Over Time', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Metric comparison\n",
    "ax = axes[1]\n",
    "metrics = ['MAE', 'RMSE', 'CRPS']\n",
    "values = [mae_score, rmse_score, mean_crps]\n",
    "colors = ['steelblue', 'orange', 'purple']\n",
    "\n",
    "bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Score (lower = better)', fontsize=11)\n",
    "ax.set_title('Forecast Evaluation Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.6f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROBABILISTIC FORECAST EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPoint Forecast Metrics (only evaluate mean):\")\n",
    "print(f\"  MAE:  {mae_score:.6f}\")\n",
    "print(f\"  RMSE: {rmse_score:.6f}\")\n",
    "print(f\"\\nProbabilistic Metric (evaluates full distribution):\")\n",
    "print(f\"  CRPS: {mean_crps:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHY CRPS MATTERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "CRPS vs MAE:\n",
    "- MAE only cares if mean forecast is close to actual\n",
    "- CRPS also cares if uncertainty is correctly specified\n",
    "\n",
    "Example:\n",
    "  Actual return: 0.01\n",
    "  \n",
    "  Forecast A: Mean = 0.005, Std = 0.02 (wide, honest uncertainty)\n",
    "  Forecast B: Mean = 0.005, Std = 0.001 (narrow, overconfident)\n",
    "  \n",
    "  MAE: Same for both (|0.01 - 0.005| = 0.005)\n",
    "  CRPS: Lower for A (correctly captures uncertainty)\n",
    "\n",
    "**Trading Application**:\n",
    "Use CRPS to select models for:\n",
    "1. Position sizing (need correct uncertainty)\n",
    "2. Option pricing (need correct distribution)\n",
    "3. Risk management (VaR/CVaR depend on tails)\n",
    "\n",
    "MAE/RMSE are okay for:\n",
    "1. Pure alpha generation (only care about directional accuracy)\n",
    "2. Benchmarking against non-probabilistic models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kelly Criterion with Bayesian Uncertainty\n",
    "\n",
    "### 3.1 Classical Kelly Criterion\n",
    "\n",
    "**Optimal position size** to maximize long-term growth rate:\n",
    "\n",
    "$$f^* = \\frac{\\mu - r}{\\sigma^2}$$\n",
    "\n",
    "Where:\n",
    "- $f^*$ = fraction of capital to invest\n",
    "- $\\mu$ = expected return\n",
    "- $r$ = risk-free rate\n",
    "- $\\sigma^2$ = variance of returns\n",
    "\n",
    "**Problem**: Assumes we **know** $\\mu$ and $\\sigma$ with certainty.\n",
    "\n",
    "### 3.2 Bayesian Kelly\n",
    "\n",
    "**Reality**: We're uncertain about $\\mu$ and $\\sigma$.\n",
    "\n",
    "**Solution**: \n",
    "$$f^* = \\mathbb{E}_{\\theta}\\left[\\frac{\\mu(\\theta) - r}{\\sigma^2(\\theta)}\\right]$$\n",
    "\n",
    "where expectation is over posterior $p(\\theta | \\text{data})$.\n",
    "\n",
    "**Alternative (more conservative)**:\n",
    "$$f^* = \\frac{\\mathbb{E}[\\mu] - r}{\\mathbb{E}[\\sigma^2] + \\text{Var}[\\mu]}$$\n",
    "\n",
    "The extra term $\\text{Var}[\\mu]$ accounts for epistemic uncertainty about expected return.\n",
    "\n",
    "### 3.3 Fractional Kelly\n",
    "\n",
    "**Full Kelly** ($f^*$) maximizes growth but has high volatility.\n",
    "\n",
    "**Fractional Kelly**: Use fraction of Kelly\n",
    "$$f_{\\text{actual}} = \\lambda f^*, \\quad \\lambda \\in [0.25, 0.5]$$\n",
    "\n",
    "**Why**:\n",
    "- Reduces volatility dramatically\n",
    "- Protects against estimation errors\n",
    "- \"Half-Kelly\" is common in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bayesian Kelly criterion\n",
    "def bayesian_kelly(trace, risk_free_rate=0.0, fraction=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Bayesian Kelly position sizing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trace : InferenceData\n",
    "        Posterior samples from Bayesian model\n",
    "    risk_free_rate : float\n",
    "        Annualized risk-free rate\n",
    "    fraction : float\n",
    "        Fraction of Kelly to use (0.5 = half-Kelly)\n",
    "    \"\"\"\n",
    "    # Extract posterior samples for mean and variance\n",
    "    # Assuming model predicts returns with mean beta * X and variance sigma^2\n",
    "    \n",
    "    # For demonstration, simulate from predictive distribution\n",
    "    # In practice, use actual posterior predictive samples\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # Simulate expected returns and volatilities from posterior\n",
    "    # (In real implementation, this comes from model)\n",
    "    expected_returns = np.random.normal(0.0005, 0.0002, n_samples)  # Epistemic uncertainty\n",
    "    volatilities = np.random.gamma(4, 0.005, n_samples)  # Uncertainty about vol\n",
    "    \n",
    "    # Calculate Kelly fraction for each posterior sample\n",
    "    kelly_fractions = (expected_returns - risk_free_rate/252) / volatilities**2\n",
    "    \n",
    "    # Conservative: average Kelly across posterior\n",
    "    mean_kelly = np.mean(kelly_fractions)\n",
    "    \n",
    "    # Apply fractional Kelly\n",
    "    actual_kelly = fraction * mean_kelly\n",
    "    \n",
    "    # Clip to reasonable range\n",
    "    actual_kelly = np.clip(actual_kelly, -1.0, 1.0)\n",
    "    \n",
    "    return {\n",
    "        'kelly_samples': kelly_fractions,\n",
    "        'mean_kelly': mean_kelly,\n",
    "        'fractional_kelly': actual_kelly,\n",
    "        'fraction_used': fraction,\n",
    "        'expected_return': np.mean(expected_returns),\n",
    "        'expected_vol': np.mean(volatilities)\n",
    "    }\n",
    "\n",
    "# Calculate Kelly for different fractions\n",
    "fractions = [0.25, 0.5, 0.75, 1.0]\n",
    "kelly_results = {}\n",
    "\n",
    "for frac in fractions:\n",
    "    # Use dummy trace (in practice, use actual model trace)\n",
    "    result = bayesian_kelly(None, risk_free_rate=0.02, fraction=frac)\n",
    "    kelly_results[frac] = result\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Kelly distribution from posterior\n",
    "ax = axes[0]\n",
    "kelly_samples = kelly_results[1.0]['kelly_samples']\n",
    "ax.hist(kelly_samples, bins=50, alpha=0.7, density=True, color='steelblue', edgecolor='black')\n",
    "ax.axvline(kelly_results[1.0]['mean_kelly'], color='red', linestyle='--', \n",
    "           linewidth=2.5, label=f\"Mean Kelly = {kelly_results[1.0]['mean_kelly']:.3f}\")\n",
    "ax.axvline(0, color='black', linestyle=':', linewidth=1.5)\n",
    "ax.set_xlabel('Kelly Fraction', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Bayesian Kelly Distribution\\n(accounting for parameter uncertainty)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fractional Kelly comparison\n",
    "ax = axes[1]\n",
    "frac_labels = ['Quarter-Kelly', 'Half-Kelly', '3/4-Kelly', 'Full Kelly']\n",
    "frac_values = [kelly_results[f]['fractional_kelly'] for f in fractions]\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "\n",
    "bars = ax.bar(frac_labels, frac_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_ylabel('Position Size (fraction of capital)', fontsize=11)\n",
    "ax.set_title('Fractional Kelly Position Sizing', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, frac_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.3f}',\n",
    "            ha='center', va='bottom' if val > 0 else 'top', \n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BAYESIAN KELLY CRITERION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExpected Return: {kelly_results[1.0]['expected_return']:.4%} (daily)\")\n",
    "print(f\"Expected Volatility: {kelly_results[1.0]['expected_vol']:.4%} (daily)\")\n",
    "print(f\"\\nOptimal Position Sizes:\")\n",
    "for frac, label in zip(fractions, frac_labels):\n",
    "    pos = kelly_results[frac]['fractional_kelly']\n",
    "    print(f\"  {label:15s}: {pos:>7.1%} of capital\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KELLY GUIDELINES FOR TRADING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. **Full Kelly is TOO AGGRESSIVE**:\n",
    "   - Maximizes long-term growth rate\n",
    "   - But has 50% probability of 50%+ drawdown!\n",
    "   - Not psychologically sustainable\n",
    "\n",
    "2. **Half-Kelly is STANDARD**:\n",
    "   - 75% of growth rate of full Kelly\n",
    "   - Only 12.5% probability of 50% drawdown\n",
    "   - Good balance of growth and volatility\n",
    "\n",
    "3. **Quarter-Kelly is CONSERVATIVE**:\n",
    "   - 50% of growth rate of full Kelly\n",
    "   - Very low drawdown risk\n",
    "   - Good for institutional mandates\n",
    "\n",
    "4. **Bayesian Kelly accounts for**:\n",
    "   - Parameter uncertainty (we don't know true μ, σ)\n",
    "   - Model uncertainty (is AR(5) the right model?)\n",
    "   - More conservative than classical Kelly\n",
    "\n",
    "**Practical Implementation**:\n",
    "- Update Kelly daily/weekly based on new forecasts\n",
    "- Use fractional Kelly (0.25 - 0.5) for safety\n",
    "- Set maximum position limits (e.g., no more than 20% in single asset)\n",
    "- Combine with portfolio-level risk budgeting\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trading Strategies with Bayesian Forecasts\n",
    "\n",
    "Now we'll implement three complete trading strategies:\n",
    "\n",
    "### 4.1 Mean Reversion with Uncertainty Bands\n",
    "\n",
    "**Idea**: Trade when price deviates significantly from forecast.\n",
    "\n",
    "**Signal**:\n",
    "- **Buy**: Price < Lower 95% band (oversold)\n",
    "- **Sell**: Price > Upper 95% band (overbought)\n",
    "- **Exit**: Price returns to forecast mean\n",
    "\n",
    "**Position sizing**: Scale by forecast uncertainty (wider bands → smaller position)\n",
    "\n",
    "### 4.2 Trend Following with Regime Detection\n",
    "\n",
    "**Idea**: Follow trends but adjust for volatility regimes.\n",
    "\n",
    "**Signal**:\n",
    "- **Trend**: Sign of forecast (positive → long, negative → short)\n",
    "- **Strength**: Magnitude of forecast relative to uncertainty\n",
    "- **Regime**: Scale position by inverse of forecasted volatility\n",
    "\n",
    "### 4.3 Pairs Trading with Bayesian Cointegration\n",
    "\n",
    "**Idea**: Trade mean-reverting spread between two correlated commodities.\n",
    "\n",
    "**Model**: \n",
    "$$\\text{Spread}_t = \\text{Price}_A - \\beta \\cdot \\text{Price}_B$$\n",
    "\n",
    "Bayesian model gives distribution of $\\beta$ (hedge ratio).\n",
    "\n",
    "**Signal**: Trade when spread exits posterior predictive interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Mean Reversion with Uncertainty Bands\n",
    "class BayesianMeanReversion:\n",
    "    \"\"\"\n",
    "    Mean reversion strategy using Bayesian forecast intervals.\n",
    "    \"\"\"\n",
    "    def __init__(self, entry_threshold=1.96, exit_threshold=0.5, \n",
    "                 max_position=0.5, volatility_scaling=True):\n",
    "        self.entry_threshold = entry_threshold  # Number of std devs for entry\n",
    "        self.exit_threshold = exit_threshold    # Number of std devs for exit\n",
    "        self.max_position = max_position\n",
    "        self.volatility_scaling = volatility_scaling\n",
    "        \n",
    "    def generate_signals(self, forecasts, prices):\n",
    "        \"\"\"\n",
    "        Generate trading signals from forecasts.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        forecasts : DataFrame\n",
    "            Must have columns: prediction, std\n",
    "        prices : Series\n",
    "            Actual prices\n",
    "        \"\"\"\n",
    "        signals = pd.DataFrame(index=forecasts.index)\n",
    "        \n",
    "        # Calculate z-score (how many std devs is price from forecast)\n",
    "        price_forecast = forecasts['prediction'].shift(1)  # Previous forecast\n",
    "        price_std = forecasts['std'].shift(1)\n",
    "        \n",
    "        # Current price relative to forecast\n",
    "        aligned_prices = prices.reindex(forecasts.index)\n",
    "        z_score = (aligned_prices - price_forecast) / price_std\n",
    "        \n",
    "        # Generate signals\n",
    "        signals['z_score'] = z_score\n",
    "        signals['signal'] = 0.0\n",
    "        \n",
    "        # Buy when price below lower band (oversold)\n",
    "        signals.loc[z_score < -self.entry_threshold, 'signal'] = 1.0\n",
    "        \n",
    "        # Sell when price above upper band (overbought)\n",
    "        signals.loc[z_score > self.entry_threshold, 'signal'] = -1.0\n",
    "        \n",
    "        # Exit when price returns to mean\n",
    "        signals.loc[abs(z_score) < self.exit_threshold, 'signal'] = 0.0\n",
    "        \n",
    "        # Position sizing: scale by inverse volatility\n",
    "        if self.volatility_scaling:\n",
    "            vol_scale = 1 / (1 + price_std / price_forecast.mean())\n",
    "            signals['position'] = signals['signal'] * vol_scale * self.max_position\n",
    "        else:\n",
    "            signals['position'] = signals['signal'] * self.max_position\n",
    "        \n",
    "        return signals\n",
    "\n",
    "# Strategy 2: Trend Following with Regime Awareness\n",
    "class BayesianTrendFollowing:\n",
    "    \"\"\"\n",
    "    Trend following with Bayesian volatility adjustment.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_confidence=0.6, max_position=0.5):\n",
    "        self.min_confidence = min_confidence\n",
    "        self.max_position = max_position\n",
    "        \n",
    "    def generate_signals(self, forecasts):\n",
    "        \"\"\"\n",
    "        Generate trend-following signals.\n",
    "        \"\"\"\n",
    "        signals = pd.DataFrame(index=forecasts.index)\n",
    "        \n",
    "        # Forecast return and uncertainty\n",
    "        forecast_return = forecasts['prediction']\n",
    "        forecast_std = forecasts['std']\n",
    "        \n",
    "        # Confidence = |forecast| / uncertainty\n",
    "        confidence = abs(forecast_return) / forecast_std\n",
    "        confidence = np.minimum(confidence, 3.0)  # Cap at 3 (very confident)\n",
    "        \n",
    "        # Signal strength based on forecast and confidence\n",
    "        signals['raw_signal'] = np.sign(forecast_return) * confidence\n",
    "        \n",
    "        # Only trade if confidence exceeds threshold\n",
    "        signals['signal'] = 0.0\n",
    "        signals.loc[confidence > self.min_confidence, 'signal'] = \\\n",
    "            signals.loc[confidence > self.min_confidence, 'raw_signal']\n",
    "        \n",
    "        # Normalize to max position\n",
    "        if signals['signal'].abs().max() > 0:\n",
    "            signals['position'] = signals['signal'] / signals['signal'].abs().max() * self.max_position\n",
    "        else:\n",
    "            signals['position'] = 0.0\n",
    "        \n",
    "        return signals\n",
    "\n",
    "# Test strategies on walk-forward forecasts\n",
    "print(\"Testing trading strategies...\\n\")\n",
    "\n",
    "# Mean Reversion\n",
    "mr_strategy = BayesianMeanReversion(entry_threshold=2.0, exit_threshold=0.5)\n",
    "mr_signals = mr_strategy.generate_signals(wf_df, data['returns'])\n",
    "\n",
    "# Trend Following  \n",
    "tf_strategy = BayesianTrendFollowing(min_confidence=0.8)\n",
    "tf_signals = tf_strategy.generate_signals(wf_df)\n",
    "\n",
    "print(\"Strategies generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest strategies\n",
    "def simple_backtest(signals, returns, initial_capital=100000, transaction_cost=0.001):\n",
    "    \"\"\"\n",
    "    Simple backtest of strategy signals.\n",
    "    \"\"\"\n",
    "    # Align signals and returns\n",
    "    common_idx = signals.index.intersection(returns.index)\n",
    "    signals_aligned = signals.reindex(common_idx)\n",
    "    returns_aligned = returns.reindex(common_idx)\n",
    "    \n",
    "    # Calculate strategy returns\n",
    "    positions = signals_aligned['position'].shift(1).fillna(0)  # Use previous signal\n",
    "    strategy_returns = positions * returns_aligned\n",
    "    \n",
    "    # Transaction costs\n",
    "    position_changes = positions.diff().abs()\n",
    "    costs = position_changes * transaction_cost\n",
    "    net_returns = strategy_returns - costs\n",
    "    \n",
    "    # Cumulative performance\n",
    "    cumulative = (1 + net_returns).cumprod() * initial_capital\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'position': positions,\n",
    "        'returns': net_returns,\n",
    "        'cumulative': cumulative\n",
    "    }, index=common_idx)\n",
    "\n",
    "# Run backtests\n",
    "mr_backtest = simple_backtest(mr_signals, data['returns'])\n",
    "tf_backtest = simple_backtest(tf_signals, data['returns'])\n",
    "\n",
    "# Buy-and-hold benchmark\n",
    "common_idx = mr_backtest.index\n",
    "bh_returns = data['returns'].reindex(common_idx)\n",
    "bh_cumulative = (1 + bh_returns).cumprod() * 100000\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Equity curves\n",
    "ax = axes[0]\n",
    "ax.plot(mr_backtest.index, mr_backtest['cumulative'], linewidth=2.5, \n",
    "        color='blue', label='Mean Reversion', alpha=0.8)\n",
    "ax.plot(tf_backtest.index, tf_backtest['cumulative'], linewidth=2.5, \n",
    "        color='green', label='Trend Following', alpha=0.8)\n",
    "ax.plot(common_idx, bh_cumulative, linewidth=2, \n",
    "        color='red', linestyle='--', label='Buy & Hold', alpha=0.6)\n",
    "ax.axhline(100000, color='black', linestyle=':', linewidth=1)\n",
    "ax.set_ylabel('Portfolio Value ($)', fontsize=11)\n",
    "ax.set_title('Strategy Performance Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Positions over time\n",
    "ax = axes[1]\n",
    "ax.plot(mr_backtest.index, mr_backtest['position'], linewidth=1.5, \n",
    "        color='blue', alpha=0.7, label='Mean Reversion')\n",
    "ax.plot(tf_backtest.index, tf_backtest['position'], linewidth=1.5, \n",
    "        color='green', alpha=0.7, label='Trend Following')\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Position Size', fontsize=11)\n",
    "ax.set_title('Position Sizes Over Time', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance metrics\n",
    "def calculate_metrics(returns, name):\n",
    "    total_return = (1 + returns).prod() - 1\n",
    "    sharpe = sharpe_ratio(returns.values)\n",
    "    max_dd = (returns.cumsum().cummax() - returns.cumsum()).max()\n",
    "    win_rate = (returns > 0).mean()\n",
    "    \n",
    "    return {\n",
    "        'Strategy': name,\n",
    "        'Total Return': f\"{total_return:.2%}\",\n",
    "        'Sharpe Ratio': f\"{sharpe:.2f}\",\n",
    "        'Max Drawdown': f\"{max_dd:.2%}\",\n",
    "        'Win Rate': f\"{win_rate:.2%}\"\n",
    "    }\n",
    "\n",
    "metrics_mr = calculate_metrics(mr_backtest['returns'], 'Mean Reversion')\n",
    "metrics_tf = calculate_metrics(tf_backtest['returns'], 'Trend Following')\n",
    "metrics_bh = calculate_metrics(bh_returns, 'Buy & Hold')\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_mr, metrics_tf, metrics_bh])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Mean Reversion:\n",
    "- Trades on deviations from forecast\n",
    "- Works well in range-bound markets\n",
    "- Scales position by forecast uncertainty (risk management)\n",
    "\n",
    "Trend Following:\n",
    "- Follows forecast direction with confidence weighting\n",
    "- Only trades when forecast is confident (low uncertainty)\n",
    "- Adapts to volatility regimes\n",
    "\n",
    "**Bayesian Advantages**:\n",
    "1. Uncertainty-aware position sizing\n",
    "2. Automatic regime detection (via forecast distribution)\n",
    "3. Natural risk management (wider intervals → smaller positions)\n",
    "4. No arbitrary parameters (thresholds derived from posteriors)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Energy Portfolio Trading System\n",
    "\n",
    "**CAPSTONE PROJECT**: Build a complete multi-asset trading system.\n",
    "\n",
    "### 5.1 System Components\n",
    "\n",
    "1. **Universe**: Crude oil, Natural gas, Heating oil (correlated energy commodities)\n",
    "2. **Models**: Bayesian VAR for multi-asset forecasting\n",
    "3. **Position Sizing**: Bayesian Kelly with portfolio constraints\n",
    "4. **Risk Management**: \n",
    "   - Portfolio VaR limit\n",
    "   - Individual asset position limits\n",
    "   - Volatility-based scaling\n",
    "5. **Execution**: Transaction costs, slippage\n",
    "6. **Monitoring**: Real-time P&L, drawdown, Sharpe\n",
    "\n",
    "### 5.2 Bayesian Vector Autoregression (VAR)\n",
    "\n",
    "**Multivariate time series model**:\n",
    "\n",
    "$$\\mathbf{y}_t = \\mathbf{A}_1 \\mathbf{y}_{t-1} + ... + \\mathbf{A}_p \\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$$\n",
    "\n",
    "where $\\mathbf{y}_t$ is vector of asset returns.\n",
    "\n",
    "**Bayesian VAR**:\n",
    "- Priors on coefficient matrices $\\mathbf{A}_i$\n",
    "- Shrinkage toward simpler models (regularization)\n",
    "- Full posterior for coefficients → uncertainty in cross-asset relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-asset energy data\n",
    "def generate_energy_portfolio_data(n=300):\n",
    "    \"\"\"\n",
    "    Simulate crude oil, natural gas, heating oil with correlations.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=n, freq='D')\n",
    "    \n",
    "    # Correlation structure\n",
    "    corr_matrix = np.array([\n",
    "        [1.0, 0.6, 0.8],   # Crude\n",
    "        [0.6, 1.0, 0.5],   # Natural gas\n",
    "        [0.8, 0.5, 1.0]    # Heating oil\n",
    "    ])\n",
    "    \n",
    "    # Cholesky decomposition for correlated normals\n",
    "    L = np.linalg.cholesky(corr_matrix)\n",
    "    \n",
    "    # Generate correlated returns\n",
    "    base_returns = np.random.normal(0, 1, (n, 3))\n",
    "    correlated_returns = base_returns @ L.T\n",
    "    \n",
    "    # Scale by different volatilities\n",
    "    crude_returns = 0.0003 + 0.02 * correlated_returns[:, 0]\n",
    "    ng_returns = 0.0002 + 0.03 * correlated_returns[:, 1]  # More volatile\n",
    "    ho_returns = 0.0004 + 0.022 * correlated_returns[:, 2]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'crude_ret': crude_returns,\n",
    "        'ng_ret': ng_returns,\n",
    "        'ho_ret': ho_returns\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Add prices\n",
    "    df['crude_price'] = 70 * np.exp(np.cumsum(crude_returns))\n",
    "    df['ng_price'] = 3.5 * np.exp(np.cumsum(ng_returns))\n",
    "    df['ho_price'] = 2.5 * np.exp(np.cumsum(ho_returns))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate portfolio data\n",
    "portfolio_data = generate_energy_portfolio_data(n=300)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
    "\n",
    "# Prices\n",
    "ax = axes[0]\n",
    "ax.plot(portfolio_data.index, portfolio_data['crude_price'], \n",
    "        linewidth=2, label='Crude Oil', alpha=0.8)\n",
    "ax.plot(portfolio_data.index, portfolio_data['ng_price'] * 15,  # Scale for visibility\n",
    "        linewidth=2, label='Natural Gas (×15)', alpha=0.8)\n",
    "ax.plot(portfolio_data.index, portfolio_data['ho_price'] * 20,  # Scale\n",
    "        linewidth=2, label='Heating Oil (×20)', alpha=0.8)\n",
    "ax.set_ylabel('Price (normalized)', fontsize=11)\n",
    "ax.set_title('Energy Portfolio: Price Evolution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation matrix\n",
    "ax = axes[1]\n",
    "returns_corr = portfolio_data[['crude_ret', 'ng_ret', 'ho_ret']].corr()\n",
    "im = ax.imshow(returns_corr, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_xticklabels(['Crude', 'Nat Gas', 'Heat Oil'])\n",
    "ax.set_yticklabels(['Crude', 'Nat Gas', 'Heat Oil'])\n",
    "ax.set_title('Return Correlations', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = ax.text(j, i, f'{returns_corr.iloc[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEnergy Portfolio Data Generated\")\n",
    "print(f\"Assets: Crude Oil, Natural Gas, Heating Oil\")\n",
    "print(f\"Period: {len(portfolio_data)} days\")\n",
    "print(f\"\\nReturn Correlations:\")\n",
    "print(returns_corr.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary: Building Production-Ready Bayesian Trading Systems\n",
    "\n",
    "### 6.1 Essential Components\n",
    "\n",
    "| Component | Bayesian Implementation |\n",
    "|-----------|------------------------|\n",
    "| **Forecasting** | GP, VAR, structural time series with full posteriors |\n",
    "| **Backtesting** | Walk-forward validation (no look-ahead bias) |\n",
    "| **Evaluation** | CRPS, calibration, log-score (not just MAE/RMSE) |\n",
    "| **Position Sizing** | Bayesian Kelly with parameter uncertainty |\n",
    "| **Risk Management** | VaR/CVaR from posterior predictive, volatility scaling |\n",
    "| **Portfolio** | Multi-asset Bayesian models (VAR, copulas) |\n",
    "\n",
    "### 6.2 What We've Learned\n",
    "\n",
    "**Module 1-3**: Bayesian foundations, priors, MCMC\n",
    "**Module 4-6**: Time series models (AR, state space, BSTS)\n",
    "**Module 7**: Hierarchical models for multiple commodities\n",
    "**Module 8**: Gaussian Processes for non-linear patterns\n",
    "**Module 9**: Volatility modeling (GARCH, stochastic vol)\n",
    "**Module 10**: Complete trading systems (this module)\n",
    "\n",
    "### 6.3 Bayesian vs Frequentist: Final Comparison\n",
    "\n",
    "**Bayesian Advantages**:\n",
    "1. ✅ Full uncertainty quantification\n",
    "2. ✅ Parameter uncertainty propagates to decisions\n",
    "3. ✅ Natural regularization through priors\n",
    "4. ✅ Sequential updating (efficient online learning)\n",
    "5. ✅ Honest risk estimates (wider intervals with limited data)\n",
    "\n",
    "**Frequentist Advantages**:\n",
    "1. ✅ Faster computation (no MCMC)\n",
    "2. ✅ Easier to implement in production\n",
    "3. ✅ Well-understood asymptotics\n",
    "\n",
    "**Recommendation**: \n",
    "- **Research/Alpha generation**: Bayesian (better forecasts)\n",
    "- **High-frequency execution**: Frequentist (speed matters)\n",
    "- **Risk management**: Bayesian (need full distributions)\n",
    "\n",
    "### 6.4 Going to Production\n",
    "\n",
    "**Infrastructure needs**:\n",
    "1. **Data pipeline**: Real-time price feeds, cleaning, storage\n",
    "2. **Model serving**: Daily refits, caching posteriors\n",
    "3. **Execution**: Order management, broker integration\n",
    "4. **Monitoring**: P&L tracking, drawdown alerts, model diagnostics\n",
    "5. **Backtesting**: Automated walk-forward validation\n",
    "\n",
    "**Common pitfalls**:\n",
    "- ❌ Overfitting in-sample\n",
    "- ❌ Ignoring transaction costs\n",
    "- ❌ Not accounting for parameter uncertainty\n",
    "- ❌ Using wrong evaluation metrics (MAE for probabilistic forecasts)\n",
    "- ❌ Over-leveraging (full Kelly is too aggressive)\n",
    "\n",
    "### 6.5 Next Steps\n",
    "\n",
    "**To deepen your knowledge**:\n",
    "1. Read: \"Bayesian Methods for Hackers\" (Cameron Davidson-Pilon)\n",
    "2. Read: \"Advances in Financial Machine Learning\" (Marcos López de Prado)\n",
    "3. Practice: Kaggle competitions with probabilistic scoring\n",
    "4. Build: Your own production system with real data\n",
    "5. Learn: Advanced topics (deep learning + Bayesian, causal inference)\n",
    "\n",
    "**Congratulations!** You've completed the Bayesian Forecasting for Commodities Trading course.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Knowledge Check Quiz\n",
    "\n",
    "**Q1**: Walk-forward validation prevents:\n",
    "- A) Model convergence issues\n",
    "- B) Look-ahead bias in backtesting\n",
    "- C) Transaction costs\n",
    "- D) Market regime changes\n",
    "\n",
    "**Q2**: CRPS is better than MAE for Bayesian forecasts because:\n",
    "- A) It's easier to calculate\n",
    "- B) It evaluates the full forecast distribution, not just the mean\n",
    "- C) It always gives higher scores\n",
    "- D) Regulators require it\n",
    "\n",
    "**Q3**: Half-Kelly position sizing means:\n",
    "- A) Use 50% of your capital\n",
    "- B) Use half the optimal Kelly fraction\n",
    "- C) Trade half as often\n",
    "- D) Split position between two assets\n",
    "\n",
    "**Q4**: Bayesian Kelly criterion accounts for:\n",
    "- A) Only expected return uncertainty\n",
    "- B) Only volatility uncertainty  \n",
    "- C) Both parameter and model uncertainty\n",
    "- D) Neither (uses point estimates)\n",
    "\n",
    "**Q5**: The main advantage of Bayesian trading systems is:\n",
    "- A) They always outperform\n",
    "- B) They're faster to execute\n",
    "- C) They quantify uncertainty for better risk management\n",
    "- D) They require less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Answers\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL QUIZ ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Q1: B) Look-ahead bias in backtesting\n",
    "    Walk-forward validation simulates real-time trading. Each forecast\n",
    "    is made using ONLY data available at that time (no future data).\n",
    "    This prevents overfitting and gives realistic performance estimates.\n",
    "\n",
    "Q2: B) It evaluates the full forecast distribution, not just the mean\n",
    "    CRPS is a proper scoring rule that rewards:\n",
    "    1. Accurate point forecasts (like MAE)\n",
    "    2. Well-calibrated uncertainty (bonus over MAE)\n",
    "    Essential for models used in risk management and position sizing.\n",
    "\n",
    "Q3: B) Use half the optimal Kelly fraction\n",
    "    Full Kelly maximizes growth but has high volatility.\n",
    "    Half-Kelly (f* × 0.5) gives:\n",
    "    - 75% of growth rate\n",
    "    - Much lower drawdowns\n",
    "    - More psychologically sustainable\n",
    "\n",
    "Q4: C) Both parameter and model uncertainty\n",
    "    Classical Kelly: assumes we KNOW μ and σ\n",
    "    Bayesian Kelly: integrates over posterior p(μ, σ | data)\n",
    "    Result: More conservative sizing when data is limited\n",
    "\n",
    "Q5: C) They quantify uncertainty for better risk management\n",
    "    Bayesian methods don't always outperform in terms of raw returns.\n",
    "    But they provide:\n",
    "    - Full predictive distributions\n",
    "    - Honest uncertainty estimates\n",
    "    - Better risk-adjusted performance\n",
    "    - Principled position sizing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Project: Build Your Own Trading System\n",
    "\n",
    "### Project Requirements\n",
    "\n",
    "Build a complete Bayesian trading system for a commodity of your choice:\n",
    "\n",
    "1. **Data**: Collect real price data (at least 2 years, daily)\n",
    "\n",
    "2. **Model**: Choose appropriate Bayesian model:\n",
    "   - Smooth trends → Gaussian Process\n",
    "   - Seasonal patterns → Structural time series\n",
    "   - Regime changes → Switching models\n",
    "\n",
    "3. **Backtesting**: Implement walk-forward validation\n",
    "   - Minimum 50 out-of-sample forecasts\n",
    "   - Calculate CRPS and calibration\n",
    "\n",
    "4. **Strategy**: Design trading strategy\n",
    "   - Mean reversion, trend following, or pairs trading\n",
    "   - Bayesian Kelly position sizing\n",
    "   - Transaction costs and slippage\n",
    "\n",
    "5. **Risk Management**:\n",
    "   - Daily VaR monitoring\n",
    "   - Maximum drawdown limits\n",
    "   - Volatility-based position scaling\n",
    "\n",
    "6. **Evaluation**:\n",
    "   - Sharpe ratio > 1.0 (after costs)\n",
    "   - Max drawdown < 20%\n",
    "   - Win rate > 50%\n",
    "\n",
    "7. **Documentation**:\n",
    "   - Model choice justification\n",
    "   - Prior selection rationale\n",
    "   - Backtesting methodology\n",
    "   - Performance analysis\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "1. Jupyter notebook with complete implementation\n",
    "2. Performance report (PDF)\n",
    "3. Production-ready code (modular, documented)\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "- **Technical (40%)**: Correct Bayesian implementation, no look-ahead bias\n",
    "- **Performance (30%)**: Risk-adjusted returns, calibration\n",
    "- **Rigor (20%)**: Proper evaluation metrics, honest reporting\n",
    "- **Presentation (10%)**: Code quality, documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Course Complete!\n",
    "\n",
    "**You've learned**:\n",
    "- Bayesian inference fundamentals\n",
    "- Time series modeling with uncertainty\n",
    "- Volatility and risk management\n",
    "- Complete trading system development\n",
    "\n",
    "**You can now**:\n",
    "- Build production Bayesian forecasting systems\n",
    "- Properly backtest without overfitting\n",
    "- Size positions optimally with Kelly criterion\n",
    "- Manage risk with probabilistic forecasts\n",
    "\n",
    "**Go forth and trade wisely!** 📈\n",
    "\n",
    "---\n",
    "\n",
    "*End of Course*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
