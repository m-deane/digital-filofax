{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: Volatility Modeling and Uncertainty Quantification\n",
    "\n",
    "**Course**: Bayesian Regression and Time Series Forecasting for Commodities Trading\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Distinguish** between epistemic (model) and aleatoric (irreducible) uncertainty\n",
    "2. **Implement** GARCH models in a Bayesian framework for volatility clustering\n",
    "3. **Build** stochastic volatility models with PyMC for time-varying risk\n",
    "4. **Calculate** Value at Risk (VaR) and Conditional VaR (CVaR) from posterior predictive distributions\n",
    "5. **Forecast** volatility regimes for risk management and position sizing\n",
    "6. **Apply** Bayesian volatility models to crude oil for trading decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Trading\n",
    "\n",
    "**Volatility is not constant**—and ignoring this can be catastrophic for traders:\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- **Crude oil (2020)**: Volatility spiked 10x during COVID, wiping out strategies calibrated to \"normal\" volatility\n",
    "- **Natural gas (Winter Storm Uri, 2021)**: Prices jumped 100x in days—fixed vol models failed completely\n",
    "- **Copper (2008)**: Volatility doubled during financial crisis; traders with constant-vol assumptions lost fortunes\n",
    "\n",
    "### Why Bayesian Volatility Modeling?\n",
    "\n",
    "1. **Time-varying risk**: Volatility clusters (high vol follows high vol)\n",
    "2. **Regime detection**: Identify when markets shift from calm to turbulent\n",
    "3. **Option pricing**: Implied vol forecasts drive delta hedging and gamma trading\n",
    "4. **Position sizing**: Scale positions by forecasted volatility (Kelly criterion)\n",
    "5. **Risk management**: VaR/CVaR for regulatory compliance and internal limits\n",
    "6. **Uncertainty decomposition**: Separate \"we don't know the model\" from \"markets are random\"\n",
    "\n",
    "**Bottom line**: Accurate volatility forecasts = better risk-adjusted returns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Two Types of Uncertainty\n",
    "\n",
    "Before modeling volatility, we must understand **what we're uncertain about**.\n",
    "\n",
    "### 1.1 Aleatoric Uncertainty (Irreducible)\n",
    "\n",
    "**Definition**: Randomness inherent in the system.\n",
    "\n",
    "- **Also called**: Statistical uncertainty, data uncertainty\n",
    "- **Source**: Markets are fundamentally stochastic\n",
    "- **Cannot be reduced**: Even with infinite data, prices are still random\n",
    "\n",
    "**Example**: \n",
    "- Crude oil price tomorrow is uncertain because of random supply/demand shocks\n",
    "- No amount of historical data will make tomorrow's price deterministic\n",
    "\n",
    "**Mathematical form**:\n",
    "$$y_t = \\mu_t + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2)$$\n",
    "\n",
    "The $\\epsilon_t$ term captures aleatoric uncertainty.\n",
    "\n",
    "### 1.2 Epistemic Uncertainty (Reducible)\n",
    "\n",
    "**Definition**: Uncertainty about the model parameters.\n",
    "\n",
    "- **Also called**: Model uncertainty, parameter uncertainty\n",
    "- **Source**: Limited data, model misspecification\n",
    "- **Can be reduced**: More data → tighter posterior → less epistemic uncertainty\n",
    "\n",
    "**Example**:\n",
    "- We're uncertain whether crude oil volatility is 20% or 30% annualized\n",
    "- More historical data narrows our belief about true volatility\n",
    "\n",
    "**Mathematical form**:\n",
    "$$\\sigma \\sim p(\\sigma | \\text{data})$$\n",
    "\n",
    "The posterior distribution $p(\\sigma | \\text{data})$ captures epistemic uncertainty.\n",
    "\n",
    "### 1.3 Total Predictive Uncertainty\n",
    "\n",
    "**Bayesian forecasts combine both**:\n",
    "\n",
    "$$p(y_{\\text{future}} | \\text{data}) = \\int p(y_{\\text{future}} | \\theta) \\cdot p(\\theta | \\text{data}) d\\theta$$\n",
    "\n",
    "- **Inner term** $p(y_{\\text{future}} | \\theta)$: Aleatoric (data randomness given parameters)\n",
    "- **Outer term** $p(\\theta | \\text{data})$: Epistemic (parameter uncertainty)\n",
    "- **Integral**: Marginalizes over parameter uncertainty\n",
    "\n",
    "**Trading implication**:\n",
    "- **Short-term forecasts**: Dominated by aleatoric uncertainty (market randomness)\n",
    "- **Long-term forecasts**: Dominated by epistemic uncertainty (don't know true parameters)\n",
    "- **Position sizing**: Use total uncertainty (both sources matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"PyMC version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate epistemic vs aleatoric uncertainty\n",
    "def demonstrate_uncertainty_types():\n",
    "    \"\"\"\n",
    "    Visualize epistemic vs aleatoric uncertainty with simple example.\n",
    "    \"\"\"\n",
    "    # Generate data from known process\n",
    "    true_mu = 100\n",
    "    true_sigma = 10\n",
    "    \n",
    "    # Two scenarios: small vs large dataset\n",
    "    n_small = 10\n",
    "    n_large = 500\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    data_small = np.random.normal(true_mu, true_sigma, n_small)\n",
    "    data_large = np.random.normal(true_mu, true_sigma, n_large)\n",
    "    \n",
    "    # Bayesian inference on mean and std\n",
    "    def infer_params(data):\n",
    "        with pm.Model() as model:\n",
    "            mu = pm.Normal('mu', mu=100, sigma=20)\n",
    "            sigma = pm.HalfNormal('sigma', sigma=15)\n",
    "            y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)\n",
    "            trace = pm.sample(1000, tune=1000, chains=2, random_seed=42, progressbar=False)\n",
    "        return trace\n",
    "    \n",
    "    trace_small = infer_params(data_small)\n",
    "    trace_large = infer_params(data_large)\n",
    "    \n",
    "    # Posterior predictive\n",
    "    def get_predictions(trace, n_pred=1000):\n",
    "        mu_samples = trace.posterior['mu'].values.flatten()\n",
    "        sigma_samples = trace.posterior['sigma'].values.flatten()\n",
    "        \n",
    "        # Sample predictions\n",
    "        n_samples = len(mu_samples)\n",
    "        predictions = np.zeros((n_samples, n_pred))\n",
    "        for i in range(n_samples):\n",
    "            predictions[i, :] = np.random.normal(mu_samples[i], sigma_samples[i], n_pred)\n",
    "        \n",
    "        return predictions.flatten()\n",
    "    \n",
    "    pred_small = get_predictions(trace_small)\n",
    "    pred_large = get_predictions(trace_large)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Small data: parameter uncertainty\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(trace_small.posterior['mu'].values.flatten(), bins=30, alpha=0.7, \n",
    "            density=True, color='orange', label=f'n={n_small}')\n",
    "    ax.axvline(true_mu, color='red', linestyle='--', linewidth=2, label='True μ')\n",
    "    ax.set_xlabel('Mean (μ)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Epistemic Uncertainty: Small Dataset', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Large data: parameter uncertainty\n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(trace_large.posterior['mu'].values.flatten(), bins=30, alpha=0.7, \n",
    "            density=True, color='green', label=f'n={n_large}')\n",
    "    ax.axvline(true_mu, color='red', linestyle='--', linewidth=2, label='True μ')\n",
    "    ax.set_xlabel('Mean (μ)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Epistemic Uncertainty: Large Dataset', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Small data: predictive distribution\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(pred_small, bins=50, alpha=0.7, density=True, color='orange', label=f'n={n_small}')\n",
    "    ax.hist(np.random.normal(true_mu, true_sigma, 10000), bins=50, alpha=0.5, \n",
    "            density=True, color='blue', label='True distribution')\n",
    "    ax.set_xlabel('Predicted Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Predictive Uncertainty: Small Dataset', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Large data: predictive distribution\n",
    "    ax = axes[1, 1]\n",
    "    ax.hist(pred_large, bins=50, alpha=0.7, density=True, color='green', label=f'n={n_large}')\n",
    "    ax.hist(np.random.normal(true_mu, true_sigma, 10000), bins=50, alpha=0.5, \n",
    "            density=True, color='blue', label='True distribution')\n",
    "    ax.set_xlabel('Predicted Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Predictive Uncertainty: Large Dataset', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantify uncertainties\n",
    "    epistemic_small = np.std(trace_small.posterior['mu'].values)\n",
    "    epistemic_large = np.std(trace_large.posterior['mu'].values)\n",
    "    aleatoric_small = np.mean(trace_small.posterior['sigma'].values)\n",
    "    aleatoric_large = np.mean(trace_large.posterior['sigma'].values)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNCERTAINTY QUANTIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSmall Dataset (n={n_small}):\")\n",
    "    print(f\"  Epistemic (parameter uncertainty): σ_μ = {epistemic_small:.2f}\")\n",
    "    print(f\"  Aleatoric (data randomness):        σ = {aleatoric_small:.2f}\")\n",
    "    print(f\"  Total predictive std:               {np.std(pred_small):.2f}\")\n",
    "    print(f\"\\nLarge Dataset (n={n_large}):\")\n",
    "    print(f\"  Epistemic (parameter uncertainty): σ_μ = {epistemic_large:.2f}\")\n",
    "    print(f\"  Aleatoric (data randomness):        σ = {aleatoric_large:.2f}\")\n",
    "    print(f\"  Total predictive std:               {np.std(pred_large):.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\"\"\n",
    "1. Epistemic uncertainty DECREASES with more data:\n",
    "   - Small dataset: σ_μ = {epistemic_small:.2f}\n",
    "   - Large dataset: σ_μ = {epistemic_large:.2f}\n",
    "   - Reduction: {epistemic_small/epistemic_large:.1f}x\n",
    "\n",
    "2. Aleatoric uncertainty STAYS CONSTANT:\n",
    "   - Small dataset: σ = {aleatoric_small:.2f}\n",
    "   - Large dataset: σ = {aleatoric_large:.2f}\n",
    "   - This is irreducible market randomness!\n",
    "\n",
    "3. Predictive uncertainty converges to aleatoric:\n",
    "   - Small data: wider (epistemic + aleatoric)\n",
    "   - Large data: narrower (mostly aleatoric)\n",
    "\n",
    "**Trading Application**:\n",
    "- New commodity with limited history → High epistemic uncertainty\n",
    "  → Use wider stop-losses, smaller positions\n",
    "- Mature commodity with decades of data → Low epistemic uncertainty\n",
    "  → Can size positions more aggressively\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_uncertainty_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GARCH Models: Volatility Clustering\n",
    "\n",
    "### 2.1 The Stylized Fact: Volatility Clusters\n",
    "\n",
    "**Observation**: Large price changes tend to follow large price changes.\n",
    "\n",
    "- Mandelbrot (1963): \"Large changes tend to be followed by large changes—of either sign—and small changes tend to be followed by small changes.\"\n",
    "- This violates the constant variance assumption of standard regression\n",
    "\n",
    "### 2.2 GARCH(1,1) Model\n",
    "\n",
    "**Generalized Autoregressive Conditional Heteroskedasticity**\n",
    "\n",
    "$$\\begin{align}\n",
    "r_t &= \\mu + \\epsilon_t \\quad \\text{(return equation)} \\\\\n",
    "\\epsilon_t &= \\sigma_t z_t, \\quad z_t \\sim \\mathcal{N}(0, 1) \\quad \\text{(standardized shock)} \\\\\n",
    "\\sigma_t^2 &= \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2 \\quad \\text{(variance equation)}\n",
    "\\end{align}$$\n",
    "\n",
    "**Parameters**:\n",
    "- $\\omega > 0$: Baseline variance\n",
    "- $\\alpha \\geq 0$: Reaction to shocks (ARCH effect)\n",
    "- $\\beta \\geq 0$: Persistence of volatility (GARCH effect)\n",
    "- **Stationarity condition**: $\\alpha + \\beta < 1$\n",
    "\n",
    "**Interpretation**:\n",
    "- $\\alpha$ high: Volatility reacts strongly to recent shocks\n",
    "- $\\beta$ high: Volatility shocks persist for long time\n",
    "- $\\alpha + \\beta \\approx 1$: Volatility shocks are nearly permanent (integrated GARCH)\n",
    "\n",
    "### 2.3 Bayesian GARCH vs Frequentist MLE\n",
    "\n",
    "**Frequentist GARCH**:\n",
    "- Maximum likelihood estimation\n",
    "- Point estimates for $\\alpha, \\beta, \\omega$\n",
    "- No parameter uncertainty\n",
    "\n",
    "**Bayesian GARCH**:\n",
    "- Full posterior distributions for parameters\n",
    "- Propagate parameter uncertainty to volatility forecasts\n",
    "- Natural shrinkage through priors\n",
    "- Can incorporate expert beliefs about volatility persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate GARCH(1,1) process\n",
    "def simulate_garch(n=500, omega=0.1, alpha=0.15, beta=0.8, mu=0.0):\n",
    "    \"\"\"\n",
    "    Simulate GARCH(1,1) returns.\n",
    "    \"\"\"\n",
    "    returns = np.zeros(n)\n",
    "    sigma2 = np.zeros(n)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)  # Unconditional variance\n",
    "    \n",
    "    for t in range(1, n):\n",
    "        # Variance equation\n",
    "        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "        \n",
    "        # Return equation\n",
    "        returns[t] = mu + np.sqrt(sigma2[t]) * np.random.randn()\n",
    "    \n",
    "    return returns, np.sqrt(sigma2)\n",
    "\n",
    "# Generate synthetic crude oil returns\n",
    "np.random.seed(42)\n",
    "returns, true_vol = simulate_garch(n=500, omega=0.05, alpha=0.12, beta=0.85, mu=0.0005)\n",
    "\n",
    "# Convert to prices\n",
    "prices = 70 * np.exp(np.cumsum(returns))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Prices\n",
    "ax = axes[0]\n",
    "ax.plot(prices, linewidth=1.5, color='black')\n",
    "ax.set_ylabel('Price ($/barrel)', fontsize=11)\n",
    "ax.set_title('Simulated Crude Oil Prices (GARCH volatility)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "ax = axes[1]\n",
    "ax.plot(returns, linewidth=1, color='blue', alpha=0.7)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_ylabel('Returns', fontsize=11)\n",
    "ax.set_title('Returns (showing volatility clustering)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility\n",
    "ax = axes[2]\n",
    "ax.plot(true_vol, linewidth=2, color='red', label='True volatility (σ_t)')\n",
    "# Rolling realized volatility (for comparison)\n",
    "rolling_vol = pd.Series(returns).rolling(window=20).std() * np.sqrt(252)\n",
    "ax.plot(rolling_vol, linewidth=1.5, color='green', alpha=0.7, label='Realized vol (20-day)')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Volatility', fontsize=11)\n",
    "ax.set_title('GARCH Volatility (time-varying)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VOLATILITY CLUSTERING EVIDENCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\" \n",
    "Notice:\n",
    "1. Periods of HIGH volatility (wide swings) cluster together\n",
    "2. Periods of LOW volatility (calm) also cluster together\n",
    "3. This is NOT captured by constant variance models!\n",
    "\n",
    "Autocorrelation of squared returns (test for ARCH effects):\n",
    "  lag-1: {np.corrcoef(returns[:-1]**2, returns[1:]**2)[0,1]:.3f}\n",
    "  lag-5: {np.corrcoef(returns[:-5]**2, returns[5:]**2)[0,1]:.3f}\n",
    "  \n",
    "Positive autocorrelation in squared returns = volatility clustering!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Bayesian GARCH(1,1)\n",
    "# Note: Full Bayesian GARCH is computationally intensive in PyMC\n",
    "# We'll use a simplified approach for demonstration\n",
    "\n",
    "def fit_bayesian_garch_simple(returns, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Simplified Bayesian GARCH using PyMC.\n",
    "    \n",
    "    For production, consider using specialized packages like\n",
    "    arch (Python) with Bayesian extensions.\n",
    "    \"\"\"\n",
    "    # Use first portion of data\n",
    "    returns_train = returns[:400]\n",
    "    \n",
    "    with pm.Model() as garch_model:\n",
    "        # Priors for GARCH parameters\n",
    "        omega = pm.HalfNormal('omega', sigma=0.1)\n",
    "        alpha = pm.Beta('alpha', alpha=2, beta=8)  # Concentrated near 0.2\n",
    "        beta = pm.Beta('beta', alpha=8, beta=2)    # Concentrated near 0.8\n",
    "        \n",
    "        # Initialize variance\n",
    "        initial_vol = pm.HalfNormal('initial_vol', sigma=0.5)\n",
    "        \n",
    "        # GARCH recursion (using scan for efficiency)\n",
    "        def garch_step(ret_prev, sigma2_prev, omega, alpha, beta):\n",
    "            sigma2_new = omega + alpha * ret_prev**2 + beta * sigma2_prev\n",
    "            return sigma2_new\n",
    "        \n",
    "        # Compute variance series\n",
    "        sigma2_series, _ = pm.scan(\n",
    "            fn=garch_step,\n",
    "            sequences=[returns_train[:-1]],\n",
    "            outputs_info=[initial_vol**2],\n",
    "            non_sequences=[omega, alpha, beta],\n",
    "            n_steps=len(returns_train)-1\n",
    "        )\n",
    "        \n",
    "        # Prepend initial variance\n",
    "        sigma2_all = pm.math.concatenate([[initial_vol**2], sigma2_series])\n",
    "        \n",
    "        # Likelihood\n",
    "        returns_obs = pm.Normal('returns_obs', \n",
    "                                mu=0, \n",
    "                                sigma=pm.math.sqrt(sigma2_all), \n",
    "                                observed=returns_train)\n",
    "        \n",
    "        # Sample posterior\n",
    "        trace = pm.sample(n_samples, tune=1000, chains=2, random_seed=42, \n",
    "                         progressbar=True, target_accept=0.95)\n",
    "    \n",
    "    return trace, garch_model\n",
    "\n",
    "print(\"Fitting Bayesian GARCH(1,1)...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "trace_garch, model_garch = fit_bayesian_garch_simple(returns, n_samples=500)\n",
    "\n",
    "print(\"\\nGARCH model fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GARCH parameter posteriors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "params = ['omega', 'alpha', 'beta']\n",
    "true_values = [0.05, 0.12, 0.85]\n",
    "param_names = ['ω (baseline var)', 'α (ARCH)', 'β (GARCH)']\n",
    "\n",
    "for ax, param, true_val, name in zip(axes, params, true_values, param_names):\n",
    "    samples = trace_garch.posterior[param].values.flatten()\n",
    "    \n",
    "    ax.hist(samples, bins=30, alpha=0.7, density=True, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(true_val, color='red', linestyle='--', linewidth=2, label=f'True = {true_val:.3f}')\n",
    "    ax.axvline(np.mean(samples), color='green', linestyle='-', linewidth=2, \n",
    "               label=f'Post mean = {np.mean(samples):.3f}')\n",
    "    ax.set_xlabel('Value', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GARCH PARAMETER POSTERIORS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Parameter':<15} {'True':>10} {'Post Mean':>12} {'Post Std':>12} {'95% CI':>25}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for param, true_val, name in zip(params, true_values, param_names):\n",
    "    samples = trace_garch.posterior[param].values.flatten()\n",
    "    mean_est = np.mean(samples)\n",
    "    std_est = np.std(samples)\n",
    "    ci = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"{name:<15} {true_val:>10.3f} {mean_est:>12.3f} {std_est:>12.3f} [{ci[0]:>8.3f}, {ci[1]:>8.3f}]\")\n",
    "\n",
    "# Check persistence\n",
    "alpha_samples = trace_garch.posterior['alpha'].values.flatten()\n",
    "beta_samples = trace_garch.posterior['beta'].values.flatten()\n",
    "persistence = alpha_samples + beta_samples\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VOLATILITY PERSISTENCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"α + β (persistence):\")\n",
    "print(f\"  Mean: {np.mean(persistence):.3f}\")\n",
    "print(f\"  95% CI: [{np.percentile(persistence, 2.5):.3f}, {np.percentile(persistence, 97.5):.3f}]\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  α + β = {np.mean(persistence):.3f} means volatility shocks have half-life of\")\n",
    "print(f\"  ~{-1/np.log(np.mean(persistence)):.1f} periods (days in this case)\")\n",
    "print(f\"\\n  Close to 1 → volatility shocks are very persistent (typical for commodities)\")\n",
    "print(f\"  Far from 1 → volatility mean-reverts quickly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stochastic Volatility Models\n",
    "\n",
    "### 3.1 Limitations of GARCH\n",
    "\n",
    "GARCH models have **deterministic volatility dynamics**:\n",
    "- Volatility is a deterministic function of past returns\n",
    "- No separate shock term for volatility itself\n",
    "\n",
    "### 3.2 Stochastic Volatility (SV) Model\n",
    "\n",
    "**Idea**: Volatility has its own random shocks.\n",
    "\n",
    "$$\\begin{align}\n",
    "r_t &= \\exp(h_t/2) \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, 1) \\quad \\text{(return equation)} \\\\\n",
    "h_t &= \\mu_h + \\phi (h_{t-1} - \\mu_h) + \\sigma_h \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, 1) \\quad \\text{(log-vol equation)}\n",
    "\\end{align}$$\n",
    "\n",
    "**Parameters**:\n",
    "- $h_t = \\log(\\sigma_t^2)$: Log-variance (ensures positivity)\n",
    "- $\\mu_h$: Mean log-variance\n",
    "- $\\phi \\in (-1, 1)$: Persistence of volatility\n",
    "- $\\sigma_h$: Volatility of volatility (vol-of-vol)\n",
    "\n",
    "**Advantages over GARCH**:\n",
    "1. **Separate volatility shocks**: $\\eta_t$ drives volatility changes\n",
    "2. **Leverage effect**: Can model correlation between $\\epsilon_t$ and $\\eta_t$ (negative correlation = leverage)\n",
    "3. **Better option pricing**: Matches implied volatility smiles\n",
    "4. **More flexible**: Captures volatility spikes not driven by returns\n",
    "\n",
    "### 3.3 Bayesian SV Estimation\n",
    "\n",
    "**Challenge**: Latent volatility $h_t$ must be inferred.\n",
    "\n",
    "**Solution**: MCMC samples both parameters $(\\mu_h, \\phi, \\sigma_h)$ and latent states $\\{h_1, ..., h_T\\}$ jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Stochastic Volatility model\n",
    "def fit_stochastic_volatility(returns, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Fit stochastic volatility model using PyMC.\n",
    "    \"\"\"\n",
    "    returns_train = returns[:400]\n",
    "    \n",
    "    with pm.Model() as sv_model:\n",
    "        # Hyperparameters\n",
    "        mu_h = pm.Normal('mu_h', mu=-3, sigma=2)  # Mean log-volatility\n",
    "        phi = pm.Uniform('phi', lower=-0.999, upper=0.999)  # AR(1) persistence\n",
    "        sigma_h = pm.HalfNormal('sigma_h', sigma=0.5)  # Vol-of-vol\n",
    "        \n",
    "        # Initial log-volatility\n",
    "        h_init = pm.Normal('h_init', mu=mu_h, sigma=sigma_h / pm.math.sqrt(1 - phi**2))\n",
    "        \n",
    "        # Log-volatility random walk (AR(1))\n",
    "        h = pm.GaussianRandomWalk('h', \n",
    "                                  mu=mu_h * (1 - phi),\n",
    "                                  sigma=sigma_h * pm.math.sqrt(1 - phi**2),\n",
    "                                  init_dist=pm.Normal.dist(mu_h, sigma_h),\n",
    "                                  steps=len(returns_train)-1)\n",
    "        \n",
    "        # Return likelihood\n",
    "        returns_obs = pm.Normal('returns_obs',\n",
    "                                mu=0,\n",
    "                                sigma=pm.math.exp(h/2),\n",
    "                                observed=returns_train)\n",
    "        \n",
    "        # Sample\n",
    "        trace = pm.sample(n_samples, tune=1000, chains=2, random_seed=42, \n",
    "                         progressbar=True, target_accept=0.9)\n",
    "    \n",
    "    return trace, sv_model\n",
    "\n",
    "print(\"Fitting Stochastic Volatility model...\")\n",
    "print(\"(This may take several minutes)\\n\")\n",
    "\n",
    "trace_sv, model_sv = fit_stochastic_volatility(returns, n_samples=500)\n",
    "\n",
    "print(\"\\nStochastic Volatility model fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize latent volatility\n",
    "h_samples = trace_sv.posterior['h'].values  # Shape: (chains, draws, time)\n",
    "h_mean = h_samples.mean(axis=(0, 1))\n",
    "h_std = h_samples.std(axis=(0, 1))\n",
    "\n",
    "# Convert log-vol to vol\n",
    "vol_mean = np.exp(h_mean / 2)\n",
    "vol_lower = np.exp((h_mean - 1.96*h_std) / 2)\n",
    "vol_upper = np.exp((h_mean + 1.96*h_std) / 2)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
    "\n",
    "# Returns\n",
    "ax = axes[0]\n",
    "ax.plot(returns[:400], linewidth=1, color='black', alpha=0.7)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_ylabel('Returns', fontsize=11)\n",
    "ax.set_title('Crude Oil Returns', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Estimated volatility\n",
    "ax = axes[1]\n",
    "ax.plot(vol_mean, linewidth=2, color='blue', label='Estimated volatility (posterior mean)')\n",
    "ax.fill_between(range(len(vol_mean)), vol_lower, vol_upper, \n",
    "                alpha=0.3, color='blue', label='95% credible interval')\n",
    "ax.plot(true_vol[:400], linewidth=2, color='red', linestyle='--', \n",
    "        alpha=0.6, label='True volatility')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Volatility (σ_t)', fontsize=11)\n",
    "ax.set_title('Stochastic Volatility Estimates', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STOCHASTIC VOLATILITY PARAMETER POSTERIORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sv_params = ['mu_h', 'phi', 'sigma_h']\n",
    "sv_names = ['Mean log-vol (μ_h)', 'Persistence (φ)', 'Vol-of-vol (σ_h)']\n",
    "\n",
    "print(f\"{'Parameter':<25} {'Mean':>10} {'Std':>10} {'95% CI':>25}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for param, name in zip(sv_params, sv_names):\n",
    "    samples = trace_sv.posterior[param].values.flatten()\n",
    "    mean_val = np.mean(samples)\n",
    "    std_val = np.std(samples)\n",
    "    ci = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"{name:<25} {mean_val:>10.3f} {std_val:>10.3f} [{ci[0]:>8.3f}, {ci[1]:>8.3f}]\")\n",
    "\n",
    "phi_mean = np.mean(trace_sv.posterior['phi'].values)\n",
    "sigma_h_mean = np.mean(trace_sv.posterior['sigma_h'].values)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Persistence (φ): {phi_mean:.3f}\n",
    "  → High persistence means volatility shocks last long\n",
    "  → Half-life: ~{-np.log(2)/np.log(phi_mean):.1f} periods\n",
    "  → Typical for commodities: φ ∈ [0.9, 0.99]\n",
    "\n",
    "Vol-of-vol (σ_h): {sigma_h_mean:.3f}\n",
    "  → Volatility itself is volatile!\n",
    "  → Higher σ_h = more abrupt volatility regime changes\n",
    "  → Important for option pricing (vega risk)\n",
    "\n",
    "**Trading Application**:\n",
    "- High φ → Volatility regimes are sticky\n",
    "  → When vol spikes, it stays high for a while\n",
    "  → Adjust position sizes for extended periods\n",
    "  \n",
    "- High σ_h → Volatility can change quickly\n",
    "  → Need frequent rebalancing\n",
    "  → Options may be mispriced (underestimate vol-of-vol)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Value at Risk (VaR) and Conditional VaR (CVaR)\n",
    "\n",
    "### 4.1 What is VaR?\n",
    "\n",
    "**Value at Risk (VaR)**: The maximum loss expected over a time horizon at a given confidence level.\n",
    "\n",
    "**Mathematical definition**:\n",
    "$$\\text{VaR}_{\\alpha}(X) = \\inf\\{x : P(X \\leq x) \\geq \\alpha\\}$$\n",
    "\n",
    "**Example**: \n",
    "- VaR at 95% confidence = 5th percentile of loss distribution\n",
    "- \"With 95% probability, losses won't exceed $X\"\n",
    "\n",
    "### 4.2 What is CVaR?\n",
    "\n",
    "**Conditional Value at Risk (CVaR)** / **Expected Shortfall (ES)**: Average loss **beyond** VaR.\n",
    "\n",
    "$$\\text{CVaR}_{\\alpha}(X) = \\mathbb{E}[X | X \\leq \\text{VaR}_{\\alpha}(X)]$$\n",
    "\n",
    "**Why CVaR > VaR**:\n",
    "- **VaR ignores tail shape**: Only cares about threshold\n",
    "- **CVaR captures tail risk**: Average of all extreme losses\n",
    "- **CVaR is coherent**: Satisfies desirable mathematical properties (subadditivity)\n",
    "\n",
    "### 4.3 Bayesian VaR/CVaR\n",
    "\n",
    "**Standard approach** (frequentist):\n",
    "1. Estimate volatility $\\hat{\\sigma}$ (point estimate)\n",
    "2. Assume normality: $\\text{VaR}_{0.05} = \\mu + \\Phi^{-1}(0.05) \\hat{\\sigma}$\n",
    "3. No uncertainty about $\\sigma$\n",
    "\n",
    "**Bayesian approach**:\n",
    "1. Sample volatility from posterior: $\\sigma \\sim p(\\sigma | \\text{data})$\n",
    "2. For each $\\sigma$ sample, generate future returns\n",
    "3. VaR/CVaR from posterior predictive distribution\n",
    "4. **Accounts for parameter uncertainty** → More conservative risk estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bayesian VaR and CVaR from SV model\n",
    "def calculate_bayesian_var_cvar(trace_sv, horizon=1, n_simulations=10000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate VaR and CVaR from stochastic volatility model.\n",
    "    \n",
    "    Accounts for:\n",
    "    1. Parameter uncertainty (from posterior)\n",
    "    2. Future volatility uncertainty (from SV dynamics)\n",
    "    3. Return randomness (aleatoric)\n",
    "    \"\"\"\n",
    "    # Extract posterior samples\n",
    "    mu_h_samples = trace_sv.posterior['mu_h'].values.flatten()\n",
    "    phi_samples = trace_sv.posterior['phi'].values.flatten()\n",
    "    sigma_h_samples = trace_sv.posterior['sigma_h'].values.flatten()\n",
    "    h_last = trace_sv.posterior['h'].values[:, :, -1].flatten()  # Last log-vol\n",
    "    \n",
    "    n_param_samples = len(mu_h_samples)\n",
    "    future_returns = np.zeros(n_simulations)\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Sample parameters from posterior\n",
    "        idx = np.random.randint(0, n_param_samples)\n",
    "        mu_h = mu_h_samples[idx]\n",
    "        phi = phi_samples[idx]\n",
    "        sigma_h = sigma_h_samples[idx]\n",
    "        h_t = h_last[idx]\n",
    "        \n",
    "        # Simulate future volatility path\n",
    "        cumulative_return = 0\n",
    "        for t in range(horizon):\n",
    "            # Update log-volatility\n",
    "            h_t = mu_h + phi * (h_t - mu_h) + sigma_h * np.random.randn()\n",
    "            \n",
    "            # Generate return\n",
    "            vol_t = np.exp(h_t / 2)\n",
    "            ret_t = vol_t * np.random.randn()\n",
    "            cumulative_return += ret_t\n",
    "        \n",
    "        future_returns[i] = cumulative_return\n",
    "    \n",
    "    # Calculate VaR and CVaR\n",
    "    alpha = 1 - confidence\n",
    "    var = np.percentile(future_returns, alpha * 100)\n",
    "    cvar = future_returns[future_returns <= var].mean()\n",
    "    \n",
    "    return future_returns, var, cvar\n",
    "\n",
    "# Calculate for different horizons\n",
    "horizons = [1, 5, 10, 20]  # 1-day, 1-week, 2-week, 1-month\n",
    "confidence = 0.95\n",
    "\n",
    "results = {}\n",
    "for h in horizons:\n",
    "    returns_sim, var, cvar = calculate_bayesian_var_cvar(trace_sv, horizon=h, \n",
    "                                                          n_simulations=5000,\n",
    "                                                          confidence=confidence)\n",
    "    results[h] = {'returns': returns_sim, 'var': var, 'cvar': cvar}\n",
    "    print(f\"Horizon {h:2d} days: VaR = {var:.4f}, CVaR = {cvar:.4f}\")\n",
    "\n",
    "print(\"\\nRisk calculations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VaR and CVaR\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, h in zip(axes.flatten(), horizons):\n",
    "    returns_sim = results[h]['returns']\n",
    "    var = results[h]['var']\n",
    "    cvar = results[h]['cvar']\n",
    "    \n",
    "    # Histogram of simulated returns\n",
    "    ax.hist(returns_sim, bins=50, alpha=0.7, density=True, color='lightblue', \n",
    "            edgecolor='black', label='Posterior predictive')\n",
    "    \n",
    "    # VaR line\n",
    "    ax.axvline(var, color='orange', linestyle='--', linewidth=2.5, \n",
    "               label=f'VaR (95%) = {var:.4f}')\n",
    "    \n",
    "    # CVaR line\n",
    "    ax.axvline(cvar, color='red', linestyle='-', linewidth=2.5, \n",
    "               label=f'CVaR (95%) = {cvar:.4f}')\n",
    "    \n",
    "    # Shade tail\n",
    "    tail_returns = returns_sim[returns_sim <= var]\n",
    "    ax.hist(tail_returns, bins=20, alpha=0.5, density=True, color='red', \n",
    "            edgecolor='darkred', label='Tail (losses > VaR)')\n",
    "    \n",
    "    ax.set_xlabel('Cumulative Return', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{h}-Day Horizon', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BAYESIAN VaR AND CVaR SUMMARY (95% Confidence)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Horizon':<10} {'VaR':>12} {'CVaR':>12} {'CVaR/VaR':>12} {'$ on $100k':>15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "portfolio_value = 100000\n",
    "\n",
    "for h in horizons:\n",
    "    var = results[h]['var']\n",
    "    cvar = results[h]['cvar']\n",
    "    ratio = cvar / var if var != 0 else np.nan\n",
    "    dollar_cvar = portfolio_value * abs(cvar)\n",
    "    \n",
    "    print(f\"{h:2d} days    {var:>12.4f} {cvar:>12.4f} {ratio:>12.2f} ${dollar_cvar:>14,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION FOR TRADERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "VaR (Value at Risk):\n",
    "  \"With 95% confidence, losses won't exceed VaR\"\n",
    "  Example: 1-day VaR = {results[1]['var']:.4f}\n",
    "  → On a $100k position, max 1-day loss is ${portfolio_value * abs(results[1]['var']):,.0f} (95% of the time)\n",
    "\n",
    "CVaR (Conditional VaR / Expected Shortfall):\n",
    "  \"Average loss in the worst 5% of cases\"\n",
    "  Example: 1-day CVaR = {results[1]['cvar']:.4f}\n",
    "  → When things go bad (worst 5%), expect to lose ${portfolio_value * abs(results[1]['cvar']):,.0f}\n",
    "\n",
    "CVaR/VaR Ratio:\n",
    "  > 1.0 means tail is fat (extreme losses are much worse than VaR)\n",
    "  = 1.0 would mean no tail risk beyond VaR\n",
    "  \n",
    "  Our ratio: ~{results[1]['cvar']/results[1]['var']:.2f}\n",
    "  → Tail risk is significant! Don't just rely on VaR.\n",
    "\n",
    "**Risk Management Actions**:\n",
    "1. Set position limits using CVaR (more conservative than VaR)\n",
    "2. Increase margin requirements in high-volatility regimes\n",
    "3. Use options to cap tail risk when CVaR/VaR ratio is high\n",
    "4. Scale positions: Position size ∝ 1/CVaR\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Volatility Forecasting for Risk Management\n",
    "\n",
    "### 5.1 Why Forecast Volatility?\n",
    "\n",
    "**Trading applications**:\n",
    "1. **Position sizing**: Higher forecast vol → smaller positions\n",
    "2. **Stop-loss placement**: Wider stops in high-vol regimes\n",
    "3. **Option strategies**: Sell vol when forecast < implied, buy when forecast > implied\n",
    "4. **Risk budgeting**: Allocate more risk capital to low-vol assets\n",
    "5. **Margin requirements**: Exchanges use vol forecasts for margin\n",
    "\n",
    "### 5.2 Multi-Step Ahead Volatility Forecasts\n",
    "\n",
    "From SV model:\n",
    "$$h_{t+k} = \\mu_h + \\phi^k (h_t - \\mu_h) + \\text{noise}$$\n",
    "\n",
    "**Key insight**: As $k \\to \\infty$, $h_{t+k} \\to \\mu_h$ (mean reversion).\n",
    "\n",
    "**Forecast variance**:\n",
    "$$\\text{Var}(h_{t+k}) = \\sigma_h^2 \\frac{1 - \\phi^{2k}}{1 - \\phi^2}$$\n",
    "\n",
    "- Short horizon: Low variance (know recent vol)\n",
    "- Long horizon: Converges to unconditional variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate volatility forecasts from SV model\n",
    "def forecast_volatility(trace_sv, n_ahead=30, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate multi-step ahead volatility forecasts.\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    mu_h_samples = trace_sv.posterior['mu_h'].values.flatten()\n",
    "    phi_samples = trace_sv.posterior['phi'].values.flatten()\n",
    "    sigma_h_samples = trace_sv.posterior['sigma_h'].values.flatten()\n",
    "    h_last = trace_sv.posterior['h'].values[:, :, -1].flatten()\n",
    "    \n",
    "    # Storage for forecasts\n",
    "    vol_forecasts = np.zeros((n_samples, n_ahead))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Sample parameters\n",
    "        idx = np.random.randint(0, len(mu_h_samples))\n",
    "        mu_h = mu_h_samples[idx]\n",
    "        phi = phi_samples[idx]\n",
    "        sigma_h = sigma_h_samples[idx]\n",
    "        h_t = h_last[idx]\n",
    "        \n",
    "        # Simulate forward\n",
    "        for t in range(n_ahead):\n",
    "            h_t = mu_h + phi * (h_t - mu_h) + sigma_h * np.random.randn()\n",
    "            vol_forecasts[i, t] = np.exp(h_t / 2)\n",
    "    \n",
    "    return vol_forecasts\n",
    "\n",
    "# Generate forecasts\n",
    "n_ahead = 60  # 2 months ahead\n",
    "vol_forecasts = forecast_volatility(trace_sv, n_ahead=n_ahead, n_samples=2000)\n",
    "\n",
    "# Summary statistics\n",
    "vol_mean = vol_forecasts.mean(axis=0)\n",
    "vol_median = np.median(vol_forecasts, axis=0)\n",
    "vol_lower = np.percentile(vol_forecasts, 5, axis=0)\n",
    "vol_upper = np.percentile(vol_forecasts, 95, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Forecast fan chart\n",
    "ax = axes[0]\n",
    "ax.plot(range(n_ahead), vol_mean, linewidth=2.5, color='blue', label='Mean forecast')\n",
    "ax.plot(range(n_ahead), vol_median, linewidth=2, color='green', \n",
    "        linestyle='--', label='Median forecast')\n",
    "ax.fill_between(range(n_ahead), vol_lower, vol_upper, \n",
    "                alpha=0.3, color='blue', label='90% prediction interval')\n",
    "ax.axhline(vol_mean[-1], color='red', linestyle=':', linewidth=2, \n",
    "           label=f'Long-run mean = {vol_mean[-1]:.4f}')\n",
    "ax.set_xlabel('Days Ahead', fontsize=11)\n",
    "ax.set_ylabel('Volatility', fontsize=11)\n",
    "ax.set_title('Volatility Forecast from Stochastic Volatility Model', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Uncertainty growth\n",
    "ax = axes[1]\n",
    "forecast_std = vol_forecasts.std(axis=0)\n",
    "ax.plot(range(n_ahead), forecast_std, linewidth=2.5, color='purple')\n",
    "ax.fill_between(range(n_ahead), 0, forecast_std, alpha=0.3, color='purple')\n",
    "ax.set_xlabel('Days Ahead', fontsize=11)\n",
    "ax.set_ylabel('Forecast Uncertainty (Std Dev)', fontsize=11)\n",
    "ax.set_title('Volatility Forecast Uncertainty Growth', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VOLATILITY FORECAST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCurrent volatility (last obs): {vol_mean[0]:.4f}\")\n",
    "print(f\"1-week ahead forecast:         {vol_mean[4]:.4f}\")\n",
    "print(f\"1-month ahead forecast:        {vol_mean[19]:.4f}\")\n",
    "print(f\"Long-run mean:                 {vol_mean[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRADING IMPLICATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "1. **Mean Reversion**:\n",
    "   - Volatility forecasts converge to long-run mean (~{vol_mean[-1]:.4f})\n",
    "   - If current vol > mean → expect decrease (sell volatility)\n",
    "   - If current vol < mean → expect increase (buy volatility)\n",
    "\n",
    "2. **Uncertainty Growth**:\n",
    "   - Forecast uncertainty increases with horizon\n",
    "   - Day 1 uncertainty: {forecast_std[0]:.4f}\n",
    "   - Day 30 uncertainty: {forecast_std[29]:.4f}\n",
    "   - Factor: {forecast_std[29]/forecast_std[0]:.1f}x higher\n",
    "\n",
    "3. **Position Sizing**:\n",
    "   - Scale position by 1/forecast_vol\n",
    "   - High vol forecast → reduce position size\n",
    "   - Low vol forecast → can increase position size\n",
    "\n",
    "4. **Option Strategies**:\n",
    "   - Compare forecast vol to implied vol\n",
    "   - If forecast < implied → sell options (vol is overpriced)\n",
    "   - If forecast > implied → buy options (vol is underpriced)\n",
    "\n",
    "5. **Stop-Loss Adjustment**:\n",
    "   - Stop distance ∝ forecast_vol\n",
    "   - High vol → wider stops (avoid noise-triggered exits)\n",
    "   - Low vol → tighter stops (protect gains)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary: Uncertainty Quantification in Practice\n",
    "\n",
    "### 6.1 Key Takeaways\n",
    "\n",
    "| Concept | Why It Matters |\n",
    "|---------|----------------|\n",
    "| **Epistemic vs Aleatoric** | Know what uncertainty you can reduce (get more data) vs what's irreducible (market randomness) |\n",
    "| **GARCH** | Volatility clusters—use GARCH to capture time-varying variance |\n",
    "| **Stochastic Volatility** | Volatility has its own random shocks—better for option pricing and tail risk |\n",
    "| **Bayesian approach** | Parameter uncertainty propagates to forecasts→more honest risk estimates |\n",
    "| **VaR vs CVaR** | CVaR captures tail risk better—use it for position limits |\n",
    "| **Volatility forecasting** | Scale positions inversely with forecast vol |\n",
    "\n",
    "### 6.2 Model Selection Guide\n",
    "\n",
    "**Use GARCH when**:\n",
    "- ✅ Need computationally fast forecasts\n",
    "- ✅ Primarily interested in point volatility forecasts\n",
    "- ✅ High-frequency trading (GARCH updates quickly)\n",
    "\n",
    "**Use Stochastic Volatility when**:\n",
    "- ✅ Pricing options (needs realistic vol dynamics)\n",
    "- ✅ Risk management (want full distribution of future vol)\n",
    "- ✅ Modeling volatility of volatility matters\n",
    "\n",
    "### 6.3 Practical Workflow\n",
    "\n",
    "1. **Model volatility** (GARCH or SV)\n",
    "2. **Forecast multi-step ahead**\n",
    "3. **Calculate VaR/CVaR** from posterior predictive\n",
    "4. **Size positions**: $w_t = \\frac{1}{\\text{CVaR}_t}$ (Kelly-like)\n",
    "5. **Set stop-losses**: $\\text{Stop distance} = 2 \\times \\text{forecast vol}$\n",
    "6. **Rebalance**: Update forecasts daily/weekly\n",
    "\n",
    "### 6.4 Common Pitfalls\n",
    "\n",
    "**❌ Assuming constant volatility**\n",
    "- Use time-varying vol models (GARCH/SV)\n",
    "\n",
    "**❌ Ignoring parameter uncertainty**\n",
    "- Bayesian approach accounts for this automatically\n",
    "\n",
    "**❌ Using only VaR**\n",
    "- VaR doesn't capture tail risk—use CVaR\n",
    "\n",
    "**❌ Over-relying on historical volatility**\n",
    "- Markets have regime changes—use priors that allow for this\n",
    "\n",
    "**❌ Not updating forecasts**\n",
    "- Volatility changes—refit models regularly (weekly for commodities)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Knowledge Check Quiz\n",
    "\n",
    "**Q1**: Epistemic uncertainty can be reduced by:\n",
    "- A) Collecting more data\n",
    "- B) Using better risk management\n",
    "- C) Diversification\n",
    "- D) It cannot be reduced\n",
    "\n",
    "**Q2**: In GARCH(1,1), high $\\alpha + \\beta$ (close to 1) means:\n",
    "- A) Volatility changes very quickly\n",
    "- B) Volatility shocks are persistent\n",
    "- C) The model is misspecified\n",
    "- D) Returns are normally distributed\n",
    "\n",
    "**Q3**: CVaR is better than VaR for risk management because:\n",
    "- A) It's easier to calculate\n",
    "- B) It captures the average loss in the tail, not just the threshold\n",
    "- C) It's always smaller than VaR\n",
    "- D) Regulators don't require it\n",
    "\n",
    "**Q4**: Stochastic Volatility models differ from GARCH by:\n",
    "- A) Being faster to estimate\n",
    "- B) Having separate random shocks for volatility\n",
    "- C) Not requiring MCMC\n",
    "- D) Always fitting better\n",
    "\n",
    "**Q5**: If a commodity has high forecasted volatility, you should:\n",
    "- A) Increase position size (more opportunity)\n",
    "- B) Decrease position size (more risk)\n",
    "- C) Keep position size constant\n",
    "- D) Exit all positions immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Answers\n",
    "print(\"=\"*70)\n",
    "print(\"QUIZ ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Q1: A) Collecting more data\n",
    "    Epistemic uncertainty is parameter/model uncertainty. More data\n",
    "    → tighter posterior → less epistemic uncertainty. Aleatoric\n",
    "    uncertainty (market randomness) cannot be reduced.\n",
    "\n",
    "Q2: B) Volatility shocks are persistent\n",
    "    α + β is the persistence parameter. Close to 1 means volatility\n",
    "    shocks decay very slowly (integrated GARCH). Typical for financial\n",
    "    time series where high-vol periods last for weeks/months.\n",
    "\n",
    "Q3: B) It captures the average loss in the tail, not just the threshold\n",
    "    VaR only tells you the threshold (e.g., 5th percentile). CVaR\n",
    "    tells you the average of all losses worse than VaR. This is much\n",
    "    more useful for risk management and is a coherent risk measure.\n",
    "\n",
    "Q4: B) Having separate random shocks for volatility\n",
    "    GARCH: σ²_t is a deterministic function of past returns\n",
    "    SV: h_t has its own random shock η_t\n",
    "    This makes SV more flexible and better for option pricing.\n",
    "\n",
    "Q5: B) Decrease position size (more risk)\n",
    "    Higher volatility = higher risk. Position sizing should be\n",
    "    inversely proportional to volatility: size ∝ 1/σ_forecast.\n",
    "    This is the foundation of risk parity and Kelly criterion.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: GARCH Parameter Sensitivity (Easy)\n",
    "Simulate GARCH(1,1) with different parameter values:\n",
    "- High α, low β (volatile but mean-reverting)\n",
    "- Low α, high β (smooth but persistent)\n",
    "- α + β = 0.99 (nearly integrated)\n",
    "\n",
    "Compare volatility dynamics and forecast accuracy.\n",
    "\n",
    "### Exercise 2: VaR Backtesting (Medium)\n",
    "1. Generate synthetic returns with time-varying vol\n",
    "2. Calculate daily VaR forecasts (95% confidence)\n",
    "3. Count how many days actual losses exceed VaR\n",
    "4. Should be ~5% for well-calibrated forecasts\n",
    "5. Test with constant vol vs GARCH vol\n",
    "\n",
    "### Exercise 3: Volatility Trading Strategy (Hard)\n",
    "Build a strategy that trades based on vol forecasts:\n",
    "1. Forecast volatility using SV model\n",
    "2. Compare to realized volatility\n",
    "3. Trade signals:\n",
    "   - If forecast_vol > realized_vol → reduce position\n",
    "   - If forecast_vol < realized_vol → increase position\n",
    "4. Backtest and calculate Sharpe ratio\n",
    "5. Compare to constant position size\n",
    "\n",
    "### Exercise 4: Option Pricing with SV (Advanced)\n",
    "Use the SV model to price European call options:\n",
    "1. Simulate future price paths from SV model\n",
    "2. Calculate option payoffs: max(S_T - K, 0)\n",
    "3. Discount to present value\n",
    "4. Compare to Black-Scholes (constant vol)\n",
    "5. Analyze implied volatility smile\n",
    "\n",
    "---\n",
    "\n",
    "## Next Module Preview\n",
    "\n",
    "In **Module 10: Backtesting, Evaluation, and Trading Strategies**, we'll bring everything together:\n",
    "- Walk-forward validation for Bayesian models\n",
    "- Proper backtesting (avoiding look-ahead bias)\n",
    "- CRPS and calibration assessment\n",
    "- Trading strategies: mean reversion, trend following, pairs trading\n",
    "- Portfolio optimization with Bayesian returns\n",
    "- Complete energy portfolio trading system\n",
    "\n",
    "---\n",
    "\n",
    "*Module 9 Complete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
