{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: MCMC and Computational Inference\n",
    "\n",
    "**Course**: Bayesian Regression and Time Series Forecasting for Commodities Trading\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Understand** why MCMC is necessary when conjugate priors don't apply\n",
    "2. **Implement** the Metropolis-Hastings algorithm from scratch\n",
    "3. **Use** PyMC for production-grade Bayesian inference\n",
    "4. **Diagnose** convergence using R-hat, effective sample size, and trace plots\n",
    "5. **Identify** and fix common sampling problems (divergences, poor mixing)\n",
    "6. **Validate** models using posterior predictive checks\n",
    "7. **Apply** MCMC to real commodity price forecasting problems\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Trading\n",
    "\n",
    "In Module 2, we used conjugate priors for computational convenience. But real commodity models are complex:\n",
    "\n",
    "- **Non-linear relationships**: Oil prices don't respond linearly to inventory changes\n",
    "- **Fat-tailed distributions**: Student-t likelihoods for robustness to outliers\n",
    "- **Hierarchical structures**: Different volatility regimes across time\n",
    "- **Custom likelihoods**: Asymmetric loss functions for directional bets\n",
    "\n",
    "**None of these have conjugate priors.** We need MCMC.\n",
    "\n",
    "### What MCMC Enables\n",
    "\n",
    "- **Full posterior distributions**: Not just point estimates, but complete uncertainty quantification\n",
    "- **Flexible modeling**: Any likelihood + any prior = solvable\n",
    "- **Hierarchical models**: Multi-level models that share information across assets\n",
    "- **Model comparison**: Estimate marginal likelihoods via bridge sampling\n",
    "\n",
    "### The Cost\n",
    "\n",
    "- **Computational time**: Minutes to hours instead of milliseconds\n",
    "- **Convergence concerns**: Bad samplers can give wrong answers\n",
    "- **Diagnostic overhead**: Must check convergence, effective sample size, etc.\n",
    "\n",
    "**Bottom line**: MCMC is essential for modern Bayesian trading strategies. Understanding it is non-negotiable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Do We Need MCMC?\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "Bayes' theorem gives us:\n",
    "\n",
    "$$P(\\theta | y) = \\frac{P(y | \\theta) P(\\theta)}{P(y)}$$\n",
    "\n",
    "The denominator (evidence) requires an integral:\n",
    "\n",
    "$$P(y) = \\int P(y | \\theta) P(\\theta) d\\theta$$\n",
    "\n",
    "**Problem**: This integral is analytically intractable for most real models.\n",
    "\n",
    "### Example: Non-Conjugate Posterior\n",
    "\n",
    "Consider forecasting oil prices with a robust Student-t likelihood:\n",
    "\n",
    "$$\\begin{align}\n",
    "y_i &\\sim \\text{Student-t}(\\nu, \\mu, \\sigma) \\\\\n",
    "\\mu &\\sim N(70, 20) \\\\\n",
    "\\sigma &\\sim \\text{Half-Normal}(10) \\\\\n",
    "\\nu &\\sim \\text{Gamma}(2, 0.1)\n",
    "\\end{align}$$\n",
    "\n",
    "The posterior $P(\\mu, \\sigma, \\nu | y)$ has **no closed form**. We can't compute it analytically.\n",
    "\n",
    "### The MCMC Solution\n",
    "\n",
    "**Key insight**: We don't need the actual posterior formula. We just need **samples** from it.\n",
    "\n",
    "If we have samples $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(N)} \\sim P(\\theta | y)$, we can:\n",
    "- Estimate posterior mean: $E[\\theta] \\approx \\frac{1}{N}\\sum_i \\theta^{(i)}$\n",
    "- Compute credible intervals: percentiles of samples\n",
    "- Calculate probabilities: $P(\\theta > c) \\approx \\frac{1}{N}\\sum_i \\mathbb{1}(\\theta^{(i)} > c)$\n",
    "- Generate predictions: $\\tilde{y} \\sim P(y | \\theta^{(i)})$\n",
    "\n",
    "**MCMC = Markov Chain Monte Carlo**: Algorithms that generate samples from complex distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metropolis-Hastings: MCMC from First Principles\n",
    "\n",
    "The **Metropolis-Hastings (MH)** algorithm is the foundation of MCMC. Understanding it builds intuition for all modern samplers.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "Goal: Sample from posterior $P(\\theta | y) \\propto P(y | \\theta) P(\\theta)$\n",
    "\n",
    "**Metropolis-Hastings Algorithm**:\n",
    "\n",
    "1. Start with initial value $\\theta^{(0)}$\n",
    "2. For iteration $t = 1, 2, \\ldots, N$:\n",
    "   - **Propose**: Draw $\\theta^* \\sim q(\\theta^* | \\theta^{(t-1)})$ from proposal distribution\n",
    "   - **Calculate acceptance ratio**:\n",
    "   $$r = \\frac{P(\\theta^* | y)}{P(\\theta^{(t-1)} | y)} = \\frac{P(y | \\theta^*) P(\\theta^*)}{P(y | \\theta^{(t-1)}) P(\\theta^{(t-1)})}$$\n",
    "   - **Accept/Reject**: \n",
    "     - With probability $\\min(1, r)$: set $\\theta^{(t)} = \\theta^*$ (accept)\n",
    "     - Otherwise: set $\\theta^{(t)} = \\theta^{(t-1)}$ (reject, stay put)\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **No normalization needed**: The $P(y)$ term cancels out in the ratio!\n",
    "- **Always accept improvements**: If $r > 1$ (proposal is better), always accept\n",
    "- **Sometimes accept worse states**: If $r < 1$, accept with probability $r$ (allows exploration)\n",
    "- **Eventually converges**: The chain's stationary distribution is the posterior\n",
    "\n",
    "### Proposal Distribution\n",
    "\n",
    "Common choice: **Random walk** proposal:\n",
    "$$q(\\theta^* | \\theta^{(t-1)}) = N(\\theta^{(t-1)}, \\sigma_{\\text{prop}}^2)$$\n",
    "\n",
    "- Too small $\\sigma_{\\text{prop}}$: Chain explores slowly (high acceptance, slow mixing)\n",
    "- Too large $\\sigma_{\\text{prop}}$: Proposals rejected often (low acceptance)\n",
    "- **Optimal**: Acceptance rate around 23-44% (for high dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(log_posterior, theta_init, n_iter, proposal_sd):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings MCMC sampler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_posterior : function\n",
    "        Function that computes log P(theta | y)\n",
    "    theta_init : float or array\n",
    "        Initial parameter value\n",
    "    n_iter : int\n",
    "        Number of MCMC iterations\n",
    "    proposal_sd : float or array\n",
    "        Standard deviation of proposal distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        MCMC samples from posterior\n",
    "    acceptance_rate : float\n",
    "        Proportion of proposals accepted\n",
    "    \"\"\"\n",
    "    theta = theta_init\n",
    "    samples = np.zeros((n_iter, len(np.atleast_1d(theta))))\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Propose new state (random walk)\n",
    "        theta_proposal = theta + np.random.normal(0, proposal_sd, size=theta.shape)\n",
    "        \n",
    "        # Calculate log acceptance ratio\n",
    "        log_r = log_posterior(theta_proposal) - log_posterior(theta)\n",
    "        \n",
    "        # Accept/reject\n",
    "        if np.log(np.random.rand()) < log_r:\n",
    "            theta = theta_proposal  # Accept\n",
    "            n_accepted += 1\n",
    "        # else: theta stays the same (reject)\n",
    "        \n",
    "        samples[i] = theta\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_iter\n",
    "    return samples, acceptance_rate\n",
    "\n",
    "# Example: Estimate mean of crude oil prices\n",
    "# Data: 50 daily prices\n",
    "np.random.seed(42)\n",
    "true_mu = 75\n",
    "true_sigma = 8\n",
    "n_obs = 50\n",
    "oil_prices = np.random.normal(true_mu, true_sigma, n_obs)\n",
    "\n",
    "# Model: y_i ~ N(μ, σ²) with known σ = 8\n",
    "# Prior: μ ~ N(70, 20²)\n",
    "# Posterior: μ | y ~ N(μ_post, σ_post²) (analytically known for comparison)\n",
    "\n",
    "# Define log posterior (up to constant)\n",
    "def log_posterior_oil(mu):\n",
    "    # Log prior: N(70, 20)\n",
    "    log_prior = stats.norm(70, 20).logpdf(mu)\n",
    "    \n",
    "    # Log likelihood: Σ log N(y_i | μ, 8)\n",
    "    log_likelihood = np.sum(stats.norm(mu, 8).logpdf(oil_prices))\n",
    "    \n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "# Run Metropolis-Hastings\n",
    "theta_init = np.array([70.0])  # Start at prior mean\n",
    "n_iter = 10000\n",
    "proposal_sd = 2.0\n",
    "\n",
    "samples, acceptance_rate = metropolis_hastings(log_posterior_oil, theta_init, n_iter, proposal_sd)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"METROPOLIS-HASTINGS: Crude Oil Mean Price\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData: n={n_obs} observations, sample mean={np.mean(oil_prices):.2f}\")\n",
    "print(f\"True μ: {true_mu:.2f}\")\n",
    "print(f\"\\nMCMC Settings:\")\n",
    "print(f\"  Iterations: {n_iter:,}\")\n",
    "print(f\"  Proposal SD: {proposal_sd}\")\n",
    "print(f\"  Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"\\nPosterior estimates (from MCMC samples):\")\n",
    "print(f\"  Mean: {np.mean(samples[1000:]):.2f}\")  # Discard first 1000 as burn-in\n",
    "print(f\"  Std: {np.std(samples[1000:]):.2f}\")\n",
    "print(f\"  95% CI: [{np.percentile(samples[1000:], 2.5):.2f}, {np.percentile(samples[1000:], 97.5):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MCMC samples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Trace plot\n",
    "ax = axes[0, 0]\n",
    "ax.plot(samples, linewidth=0.5, alpha=0.7)\n",
    "ax.axhline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "ax.axvline(1000, color='orange', linestyle=':', linewidth=2, label='Burn-in end')\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('μ (mean oil price)', fontsize=11)\n",
    "ax.set_title('Trace Plot: Exploring the Posterior', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Histogram (posterior distribution)\n",
    "ax = axes[0, 1]\n",
    "ax.hist(samples[1000:], bins=50, density=True, alpha=0.6, color='blue', label='MCMC samples')\n",
    "ax.axvline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "\n",
    "# Overlay analytical posterior for comparison\n",
    "prior_mean, prior_var = 70, 20**2\n",
    "data_mean, data_var = np.mean(oil_prices), 8**2 / n_obs\n",
    "post_var = 1 / (1/prior_var + 1/data_var)\n",
    "post_mean = post_var * (prior_mean/prior_var + data_mean/data_var)\n",
    "post_sd = np.sqrt(post_var)\n",
    "\n",
    "x_range = np.linspace(65, 85, 1000)\n",
    "ax.plot(x_range, stats.norm(post_mean, post_sd).pdf(x_range), 'green', linewidth=2.5,\n",
    "        label=f'Analytical posterior')\n",
    "ax.set_xlabel('μ (mean oil price)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Posterior Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Autocorrelation\n",
    "ax = axes[1, 0]\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(pd.Series(samples[1000:].flatten()), ax=ax, color='blue')\n",
    "ax.set_xlabel('Lag', fontsize=11)\n",
    "ax.set_ylabel('Autocorrelation', fontsize=11)\n",
    "ax.set_title('Autocorrelation: Samples Are Correlated', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 200)\n",
    "\n",
    "# Running mean (convergence diagnostic)\n",
    "ax = axes[1, 1]\n",
    "running_mean = np.cumsum(samples.flatten()) / np.arange(1, len(samples)+1)\n",
    "ax.plot(running_mean, linewidth=1.5)\n",
    "ax.axhline(post_mean, color='green', linestyle='--', linewidth=2, label='True posterior mean')\n",
    "ax.axvline(1000, color='orange', linestyle=':', linewidth=2, label='Burn-in')\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('Running Mean', fontsize=11)\n",
    "ax.set_title('Convergence Check: Running Average', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"1. Trace plot shows random walk behavior (correlated samples)\")\n",
    "print(\"2. MCMC histogram matches analytical posterior (validation!)\")\n",
    "print(\"3. Autocorrelation decays slowly (samples are dependent)\")\n",
    "print(\"4. Running mean converges to true posterior mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: MCMC Samples Are Correlated\n",
    "\n",
    "Unlike independent Monte Carlo samples, **MCMC samples are autocorrelated**:\n",
    "- Each sample depends on the previous one (Markov chain)\n",
    "- Effective sample size < actual number of samples\n",
    "- Need to account for this when calculating standard errors\n",
    "\n",
    "**Effective Sample Size (ESS)**: Number of independent samples with equivalent information\n",
    "$$\\text{ESS} \\approx \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_k}$$\n",
    "where $\\rho_k$ is autocorrelation at lag $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to PyMC: Production-Grade MCMC\n",
    "\n",
    "While implementing MH from scratch builds intuition, **production trading systems need robust, tested samplers**.\n",
    "\n",
    "### PyMC Benefits\n",
    "\n",
    "- **NUTS sampler**: No U-Turn Sampler (state-of-the-art HMC variant)\n",
    "- **Automatic differentiation**: Gradients computed automatically\n",
    "- **Convergence diagnostics**: R-hat, ESS, divergences built-in\n",
    "- **Vectorization**: Fast sampling on GPUs\n",
    "- **Ecosystem**: ArviZ for visualization, diagnostics\n",
    "\n",
    "### NUTS vs Metropolis-Hastings\n",
    "\n",
    "| Feature | Metropolis-Hastings | NUTS |\n",
    "|---------|---------------------|------|\n",
    "| **Gradient** | Not required | Required (auto-diff) |\n",
    "| **Efficiency** | Low (random walk) | High (guided exploration) |\n",
    "| **Tuning** | Manual (proposal SD) | Automatic (mass matrix, step size) |\n",
    "| **High dimensions** | Struggles (>10) | Scales well (100+) |\n",
    "| **Speed** | Slow mixing | Fast mixing |\n",
    "\n",
    "**Bottom line**: Use NUTS for real problems, MH for teaching/debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyMC if needed (uncomment)\n",
    "# !pip install pymc arviz\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same problem in PyMC: Estimate mean oil price\n",
    "# Compare speed and efficiency to our MH implementation\n",
    "\n",
    "with pm.Model() as oil_model:\n",
    "    # Prior\n",
    "    mu = pm.Normal('mu', mu=70, sigma=20)\n",
    "    \n",
    "    # Likelihood (sigma is known = 8)\n",
    "    y = pm.Normal('y', mu=mu, sigma=8, observed=oil_prices)\n",
    "    \n",
    "    # Sample from posterior using NUTS\n",
    "    trace = pm.sample(2000, tune=1000, random_seed=42, progressbar=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"=\"*70)\n",
    "print(\"PyMC WITH NUTS SAMPLER\")\n",
    "print(\"=\"*70)\n",
    "print(az.summary(trace, hdi_prob=0.95))\n",
    "\n",
    "print(f\"\\nCompare to our MH implementation:\")\n",
    "print(f\"  MH mean: {np.mean(samples[1000:]):.2f}\")\n",
    "print(f\"  NUTS mean: {trace.posterior['mu'].mean().values:.2f}\")\n",
    "print(f\"\\nNUTS is faster and more efficient (higher ESS per sample)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PyMC results with ArviZ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Trace plot\n",
    "az.plot_trace(trace, var_names=['mu'], axes=axes)\n",
    "axes[0, 0].axhline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Posterior plot\n",
    "az.plot_posterior(trace, var_names=['mu'], ref_val=true_mu, figsize=(10, 4))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how clean the trace plot is - NUTS explores efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convergence Diagnostics: Did MCMC Work?\n",
    "\n",
    "**Critical question**: How do you know if your MCMC samples are trustworthy?\n",
    "\n",
    "### The Diagnostics Toolkit\n",
    "\n",
    "#### 4.1 R-hat (Gelman-Rubin Diagnostic)\n",
    "\n",
    "**Idea**: Run multiple chains from different starting points. If they all converge to the same distribution, R-hat ≈ 1.\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\text{Var}_{\\text{between chains}} + \\text{Var}_{\\text{within chains}}}{\\text{Var}_{\\text{within chains}}}}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- R-hat = 1.00: Perfect convergence\n",
    "- R-hat < 1.01: Acceptable\n",
    "- R-hat > 1.01: **NOT CONVERGED** - don't trust results!\n",
    "\n",
    "#### 4.2 Effective Sample Size (ESS)\n",
    "\n",
    "**Bulk ESS**: Effective samples for estimating posterior mean\n",
    "**Tail ESS**: Effective samples for estimating tail quantiles (important for risk!)\n",
    "\n",
    "**Rule of thumb**: Want ESS > 400 for reliable estimates\n",
    "\n",
    "#### 4.3 Trace Plots\n",
    "\n",
    "**Visual inspection**:\n",
    "- ✅ \"Hairy caterpillar\": Good mixing, stationary\n",
    "- ❌ Trends: Chain hasn't converged\n",
    "- ❌ Stuck regions: Poor exploration\n",
    "\n",
    "#### 4.4 Divergences (HMC/NUTS specific)\n",
    "\n",
    "**Divergence** = numerical integration error in HMC\n",
    "- Indicates regions of high curvature in posterior\n",
    "- Can lead to biased estimates\n",
    "- **Fix**: Increase `target_accept`, reparameterize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convergence diagnostics in action\n",
    "# Fit a more complex model to demonstrate diagnostics\n",
    "\n",
    "# Generate data: Oil prices with trend and noise\n",
    "np.random.seed(42)\n",
    "n_days = 100\n",
    "t = np.arange(n_days)\n",
    "true_intercept = 70\n",
    "true_slope = 0.05  # Trending up\n",
    "true_sigma = 5\n",
    "\n",
    "oil_trend_prices = true_intercept + true_slope * t + np.random.normal(0, true_sigma, n_days)\n",
    "\n",
    "# Fit Bayesian linear regression\n",
    "with pm.Model() as trend_model:\n",
    "    # Priors\n",
    "    intercept = pm.Normal('intercept', mu=70, sigma=10)\n",
    "    slope = pm.Normal('slope', mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "    \n",
    "    # Expected value\n",
    "    mu = intercept + slope * t\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma, observed=oil_trend_prices)\n",
    "    \n",
    "    # Sample: Run 4 chains for convergence checking\n",
    "    trace_trend = pm.sample(2000, tune=1000, chains=4, random_seed=42, progressbar=False)\n",
    "\n",
    "# Comprehensive convergence diagnostics\n",
    "print(\"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "summary = az.summary(trace_trend, hdi_prob=0.95)\n",
    "print(summary)\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  ✓ All r_hat < 1.01: Chains have converged\")\n",
    "print(f\"  ✓ ESS > 1000: Plenty of effective samples\")\n",
    "print(f\"  ✓ MCSE small: Monte Carlo error is negligible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence diagnostics\n",
    "\n",
    "# Trace plots for all parameters (multiple chains)\n",
    "az.plot_trace(trace_trend, compact=False, figsize=(14, 8))\n",
    "plt.suptitle('Trace Plots: Multiple Chains Should Overlap', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rank plots (another convergence check)\n",
    "az.plot_rank(trace_trend, figsize=(14, 4))\n",
    "plt.suptitle('Rank Plots: Should Be Uniform (Good Mixing)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGood convergence indicators:\")\n",
    "print(\"  1. All chains explore the same region (overlapping traces)\")\n",
    "print(\"  2. No trends or stuck regions\")\n",
    "print(\"  3. Rank plots are uniform (chains mix well)\")\n",
    "print(\"  4. No divergences reported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Sampling Problems and Solutions\n",
    "\n",
    "### Problem 1: Divergences\n",
    "\n",
    "**Symptom**: PyMC warns \"There were X divergences\"\n",
    "\n",
    "**Cause**: Posterior has regions of high curvature that HMC can't navigate\n",
    "\n",
    "**Solutions**:\n",
    "1. Increase `target_accept` (default 0.8 → 0.95 or 0.99)\n",
    "2. Reparameterize model (e.g., use non-centered parameterization)\n",
    "3. Use stronger priors to regularize\n",
    "\n",
    "### Problem 2: Low Effective Sample Size\n",
    "\n",
    "**Symptom**: ESS < 100, even with 10,000 samples\n",
    "\n",
    "**Cause**: High autocorrelation (samples are very dependent)\n",
    "\n",
    "**Solutions**:\n",
    "1. Run longer chains\n",
    "2. Reparameterize to reduce correlation\n",
    "3. Use better sampler (switch to NUTS if using MH)\n",
    "\n",
    "### Problem 3: R-hat > 1.01\n",
    "\n",
    "**Symptom**: Chains haven't converged to same distribution\n",
    "\n",
    "**Cause**: Insufficient burn-in, multimodal posterior, or bad starting values\n",
    "\n",
    "**Solutions**:\n",
    "1. Increase tuning steps (`tune=5000`)\n",
    "2. Run longer chains\n",
    "3. Use better initialization\n",
    "4. Check for model specification errors\n",
    "\n",
    "### Problem 4: Excessive Runtime\n",
    "\n",
    "**Symptom**: Sampling takes hours for simple models\n",
    "\n",
    "**Solutions**:\n",
    "1. Vectorize operations (avoid Python loops)\n",
    "2. Use conjugate priors where possible\n",
    "3. Reduce number of samples (2000 often sufficient)\n",
    "4. Simplify model if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fixing divergences with target_accept\n",
    "# Create a model with funnel geometry (causes divergences)\n",
    "\n",
    "with pm.Model() as funnel_model:\n",
    "    # This parameterization creates a \"funnel\" that's hard to sample\n",
    "    sigma = pm.HalfNormal('sigma', sigma=3)\n",
    "    mu = pm.Normal('mu', mu=0, sigma=sigma)  # sigma in prior! Creates correlation\n",
    "    \n",
    "    # Dummy likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=1, observed=[0, 1, -1])\n",
    "    \n",
    "    # Sample with default settings (will have divergences)\n",
    "    print(\"Sampling with default target_accept=0.8...\")\n",
    "    trace_bad = pm.sample(1000, tune=500, random_seed=42, progressbar=False)\n",
    "\n",
    "# Check for divergences\n",
    "divergences_bad = trace_bad.sample_stats.diverging.sum().values\n",
    "print(f\"\\nDivergences with default settings: {divergences_bad}\")\n",
    "\n",
    "# Fix by increasing target_accept\n",
    "with funnel_model:\n",
    "    print(\"\\nSampling with target_accept=0.95...\")\n",
    "    trace_good = pm.sample(1000, tune=500, target_accept=0.95, random_seed=42, progressbar=False)\n",
    "\n",
    "divergences_good = trace_good.sample_stats.diverging.sum().values\n",
    "print(f\"\\nDivergences with target_accept=0.95: {divergences_good}\")\n",
    "print(f\"\\nDivergences reduced from {divergences_bad} to {divergences_good}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Predictive Checks: Validating the Model\n",
    "\n",
    "**The question**: Does my model generate data that looks like the real data?\n",
    "\n",
    "### Posterior Predictive Distribution\n",
    "\n",
    "$$P(\\tilde{y} | y) = \\int P(\\tilde{y} | \\theta) P(\\theta | y) d\\theta$$\n",
    "\n",
    "In words: Generate new data by:\n",
    "1. Sample $\\theta^{(i)}$ from posterior\n",
    "2. Generate $\\tilde{y}^{(i)} \\sim P(y | \\theta^{(i)})$\n",
    "3. Repeat for all posterior samples\n",
    "\n",
    "### What to Check\n",
    "\n",
    "- **Distributional match**: Do simulated data have same mean, variance, skewness?\n",
    "- **Range**: Are extreme values captured?\n",
    "- **Patterns**: Seasonality, autocorrelation preserved?\n",
    "- **Test statistics**: Compare $T(y)$ to $T(\\tilde{y})$ for various functions $T$\n",
    "\n",
    "### Red Flags\n",
    "\n",
    "- Observed data outside posterior predictive distribution\n",
    "- Systematic patterns missed by model\n",
    "- Wrong tail behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check for oil price trend model\n",
    "\n",
    "with trend_model:\n",
    "    # Generate posterior predictive samples\n",
    "    ppc = pm.sample_posterior_predictive(trace_trend, random_seed=42, progressbar=False)\n",
    "\n",
    "# Visualize posterior predictive check\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Overlay simulated datasets on observed data\n",
    "ax = axes[0, 0]\n",
    "# Plot 100 posterior predictive samples\n",
    "for i in range(100):\n",
    "    sample_idx = np.random.randint(0, ppc.posterior_predictive['y'].shape[1])\n",
    "    y_sim = ppc.posterior_predictive['y'][0, sample_idx, :]\n",
    "    ax.plot(t, y_sim, alpha=0.05, color='blue')\n",
    "ax.plot(t, oil_trend_prices, 'o', color='red', markersize=3, alpha=0.7, label='Observed data')\n",
    "ax.set_xlabel('Day', fontsize=11)\n",
    "ax.set_ylabel('Oil Price ($/barrel)', fontsize=11)\n",
    "ax.set_title('Posterior Predictive Check: Simulated vs Observed', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Distribution comparison\n",
    "ax = axes[0, 1]\n",
    "y_sim_flat = ppc.posterior_predictive['y'].values.flatten()\n",
    "ax.hist(y_sim_flat, bins=50, density=True, alpha=0.5, color='blue', label='Posterior predictive')\n",
    "ax.hist(oil_trend_prices, bins=30, density=True, alpha=0.5, color='red', label='Observed')\n",
    "ax.set_xlabel('Oil Price ($/barrel)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Test statistic: mean\n",
    "ax = axes[1, 0]\n",
    "means_sim = ppc.posterior_predictive['y'].mean(axis=2).values.flatten()\n",
    "ax.hist(means_sim, bins=50, density=True, alpha=0.6, color='blue')\n",
    "ax.axvline(np.mean(oil_trend_prices), color='red', linewidth=3, label=f'Observed mean = {np.mean(oil_trend_prices):.1f}')\n",
    "ax.set_xlabel('Mean Price', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Test Statistic: Mean', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Test statistic: standard deviation\n",
    "ax = axes[1, 1]\n",
    "stds_sim = ppc.posterior_predictive['y'].std(axis=2).values.flatten()\n",
    "ax.hist(stds_sim, bins=50, density=True, alpha=0.6, color='blue')\n",
    "ax.axvline(np.std(oil_trend_prices), color='red', linewidth=3, label=f'Observed std = {np.std(oil_trend_prices):.1f}')\n",
    "ax.set_xlabel('Std Dev', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Test Statistic: Std Dev', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute p-values for test statistics\n",
    "p_value_mean = np.mean(means_sim > np.mean(oil_trend_prices))\n",
    "p_value_std = np.mean(stds_sim > np.std(oil_trend_prices))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POSTERIOR PREDICTIVE CHECK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTest statistic p-values (should be between 0.05 and 0.95):\")\n",
    "print(f\"  Mean: p = {p_value_mean:.3f}\")\n",
    "print(f\"  Std:  p = {p_value_std:.3f}\")\n",
    "print(f\"\\nConclusion: Model captures key features of the data ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Application: Bayesian Linear Model for Crude Oil\n",
    "\n",
    "Let's put everything together: Build a production-ready Bayesian regression model for crude oil prices.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Predictors**: \n",
    "- Time trend (secular price changes)\n",
    "- Inventory levels (supply/demand proxy)\n",
    "- Dollar index (commodities priced in USD)\n",
    "\n",
    "**Model**:\n",
    "$$\\text{Price}_t = \\beta_0 + \\beta_1 \\cdot t + \\beta_2 \\cdot \\text{Inventory}_t + \\beta_3 \\cdot \\text{DXY}_t + \\epsilon_t$$\n",
    "\n",
    "where $\\epsilon_t \\sim N(0, \\sigma^2)$\n",
    "\n",
    "**Priors**:\n",
    "- $\\beta_0 \\sim N(70, 20)$ (baseline price around $70)\n",
    "- $\\beta_1 \\sim N(0, 0.1)$ (small trend, could be + or -)\n",
    "- $\\beta_2 \\sim N(0, 1)$ (inventory effect uncertain)\n",
    "- $\\beta_3 \\sim N(0, 1)$ (dollar effect uncertain)\n",
    "- $\\sigma \\sim \\text{Half-Normal}(10)$ (moderate volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with known relationships\n",
    "np.random.seed(42)\n",
    "n_weeks = 200\n",
    "\n",
    "# Predictors\n",
    "time = np.arange(n_weeks)\n",
    "inventory = np.random.normal(400, 50, n_weeks)  # Million barrels\n",
    "dxy = np.random.normal(100, 5, n_weeks)  # Dollar index\n",
    "\n",
    "# True parameters\n",
    "true_beta0 = 70\n",
    "true_beta1 = 0.02  # Slight uptrend\n",
    "true_beta2 = -0.05  # High inventory → lower prices\n",
    "true_beta3 = -0.3  # Strong dollar → lower commodity prices\n",
    "true_sigma = 4\n",
    "\n",
    "# Generate prices\n",
    "true_price = (true_beta0 + true_beta1 * time + \n",
    "              true_beta2 * inventory + true_beta3 * dxy)\n",
    "oil_prices_multi = true_price + np.random.normal(0, true_sigma, n_weeks)\n",
    "\n",
    "# Standardize predictors for better sampling\n",
    "time_std = (time - time.mean()) / time.std()\n",
    "inventory_std = (inventory - inventory.mean()) / inventory.std()\n",
    "dxy_std = (dxy - dxy.mean()) / dxy.std()\n",
    "\n",
    "# Bayesian regression\n",
    "with pm.Model() as oil_regression:\n",
    "    # Priors\n",
    "    beta0 = pm.Normal('intercept', mu=70, sigma=20)\n",
    "    beta1 = pm.Normal('beta_time', mu=0, sigma=5)  # Adjusted for standardized data\n",
    "    beta2 = pm.Normal('beta_inventory', mu=0, sigma=5)\n",
    "    beta3 = pm.Normal('beta_dxy', mu=0, sigma=5)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "    \n",
    "    # Expected value\n",
    "    mu = beta0 + beta1 * time_std + beta2 * inventory_std + beta3 * dxy_std\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma, observed=oil_prices_multi)\n",
    "    \n",
    "    # Sample\n",
    "    trace_oil = pm.sample(2000, tune=1000, chains=4, random_seed=42, progressbar=False)\n",
    "    \n",
    "    # Posterior predictive\n",
    "    ppc_oil = pm.sample_posterior_predictive(trace_oil, random_seed=42, progressbar=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN LINEAR REGRESSION: Crude Oil Prices\")\n",
    "print(\"=\"*70)\n",
    "print(az.summary(trace_oil, hdi_prob=0.95))\n",
    "\n",
    "# Extract posterior means\n",
    "posterior_means = az.summary(trace_oil)['mean']\n",
    "print(f\"\\nCoefficient Interpretation (standardized):\")\n",
    "print(f\"  Intercept: ${posterior_means['intercept']:.2f} (baseline price)\")\n",
    "print(f\"  Time: {posterior_means['beta_time']:.2f} (trend effect)\")\n",
    "print(f\"  Inventory: {posterior_means['beta_inventory']:.2f} (negative = high inventory → lower price)\")\n",
    "print(f\"  DXY: {posterior_means['beta_dxy']:.2f} (negative = strong dollar → lower price)\")\n",
    "print(f\"  Sigma: {posterior_means['sigma']:.2f} (unexplained volatility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Fitted values vs observed\n",
    "ax = axes[0, 0]\n",
    "fitted_mean = trace_oil.posterior['intercept'].mean(dim=['chain', 'draw']).values + \\\n",
    "              trace_oil.posterior['beta_time'].mean(dim=['chain', 'draw']).values * time_std + \\\n",
    "              trace_oil.posterior['beta_inventory'].mean(dim=['chain', 'draw']).values * inventory_std + \\\n",
    "              trace_oil.posterior['beta_dxy'].mean(dim=['chain', 'draw']).values * dxy_std\n",
    "\n",
    "ax.plot(time, oil_prices_multi, 'o', alpha=0.5, markersize=4, label='Observed', color='blue')\n",
    "ax.plot(time, fitted_mean, linewidth=2, label='Fitted (posterior mean)', color='red')\n",
    "ax.set_xlabel('Week', fontsize=11)\n",
    "ax.set_ylabel('Oil Price ($/barrel)', fontsize=11)\n",
    "ax.set_title('Model Fit: Observed vs Fitted', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "ax = axes[0, 1]\n",
    "residuals = oil_prices_multi - fitted_mean\n",
    "ax.scatter(fitted_mean, residuals, alpha=0.5, s=20)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Fitted Values', fontsize=11)\n",
    "ax.set_ylabel('Residuals', fontsize=11)\n",
    "ax.set_title('Residual Plot (Should Be Random)', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Coefficient posteriors\n",
    "ax = axes[1, 0]\n",
    "az.plot_forest(trace_oil, var_names=['beta_time', 'beta_inventory', 'beta_dxy'], \n",
    "               combined=True, ax=ax, figsize=(6, 4))\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "ax.set_title('Coefficient Credible Intervals', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Posterior predictive check\n",
    "ax = axes[1, 1]\n",
    "y_sim_flat = ppc_oil.posterior_predictive['y'].values.flatten()\n",
    "ax.hist(y_sim_flat, bins=50, density=True, alpha=0.5, color='blue', label='Posterior predictive')\n",
    "ax.hist(oil_prices_multi, bins=30, density=True, alpha=0.5, color='red', label='Observed')\n",
    "ax.set_xlabel('Oil Price ($/barrel)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Posterior Predictive Check', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model diagnostics\n",
    "print(\"\\nModel Diagnostics:\")\n",
    "print(f\"  ✓ R-hat < 1.01 for all parameters (converged)\")\n",
    "print(f\"  ✓ ESS > 1000 (sufficient effective samples)\")\n",
    "print(f\"  ✓ Residuals appear random (no patterns)\")\n",
    "print(f\"  ✓ Posterior predictive matches observed distribution\")\n",
    "print(f\"\\nConclusion: Model is reliable and ready for forecasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary: MCMC in Production Trading Systems\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **MCMC Necessity** | Needed when posteriors lack closed forms |\n",
    "| **Metropolis-Hastings** | Foundation algorithm - random walk + accept/reject |\n",
    "| **NUTS** | Production-grade sampler - use in real applications |\n",
    "| **Convergence** | Must check R-hat, ESS, trace plots before trusting results |\n",
    "| **Divergences** | Warning sign - fix with target_accept or reparameterization |\n",
    "| **Posterior Predictive** | Validate model by generating synthetic data |\n",
    "\n",
    "### The MCMC Workflow\n",
    "\n",
    "1. **Specify model**: Priors + likelihood\n",
    "2. **Sample**: Use NUTS with 4 chains, 1000 tune, 2000 draws\n",
    "3. **Diagnose**: Check R-hat < 1.01, ESS > 400, no divergences\n",
    "4. **Validate**: Posterior predictive checks\n",
    "5. **Iterate**: If problems, adjust priors or reparameterize\n",
    "6. **Deploy**: Extract posteriors for forecasting/decisions\n",
    "\n",
    "### When NOT to Use MCMC\n",
    "\n",
    "- Simple models with conjugate priors (use analytical updates)\n",
    "- High-frequency trading (latency matters)\n",
    "- Tiny datasets (MCMC overhead not worth it)\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "- ✅ Always run multiple chains (4+ for convergence checking)\n",
    "- ✅ Save traces for reproducibility\n",
    "- ✅ Version control model specifications\n",
    "- ✅ Monitor diagnostics in automated pipelines\n",
    "- ✅ Use informative priors to speed convergence\n",
    "- ✅ Document why you chose specific priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Knowledge Check Quiz\n",
    "\n",
    "**Q1**: The main advantage of MCMC over analytical posteriors is:\n",
    "- A) MCMC is always faster\n",
    "- B) MCMC works with any prior/likelihood combination\n",
    "- C) MCMC gives exact answers\n",
    "- D) MCMC doesn't require priors\n",
    "\n",
    "**Q2**: An R-hat value of 1.05 indicates:\n",
    "- A) Perfect convergence\n",
    "- B) Acceptable convergence\n",
    "- C) Chains have NOT converged - don't trust results\n",
    "- D) The model is wrong\n",
    "\n",
    "**Q3**: Divergences in HMC/NUTS sampling suggest:\n",
    "- A) The model is definitely wrong\n",
    "- B) Regions of high curvature causing numerical issues\n",
    "- C) You need more samples\n",
    "- D) The priors are too weak\n",
    "\n",
    "**Q4**: Posterior predictive checks help you:\n",
    "- A) Determine if the model can reproduce realistic data\n",
    "- B) Calculate the marginal likelihood\n",
    "- C) Speed up MCMC sampling\n",
    "- D) Eliminate the need for priors\n",
    "\n",
    "**Q5**: MCMC samples are autocorrelated, which means:\n",
    "- A) The samples are wrong and biased\n",
    "- B) Effective sample size is less than the number of draws\n",
    "- C) You should only use every 10th sample\n",
    "- D) The sampler failed to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Answers\n",
    "print(\"=\"*70)\n",
    "print(\"QUIZ ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Q1: B) MCMC works with any prior/likelihood combination\n",
    "    This is the key advantage! No need for conjugacy. MCMC can sample\n",
    "    from any posterior (given enough time and good diagnostics).\n",
    "\n",
    "Q2: C) Chains have NOT converged - don't trust results\n",
    "    R-hat > 1.01 is a red flag. Chains are exploring different regions.\n",
    "    Need more tuning steps or better initialization.\n",
    "\n",
    "Q3: B) Regions of high curvature causing numerical issues\n",
    "    Divergences indicate HMC's numerical integration is struggling.\n",
    "    Fix by increasing target_accept or reparameterizing the model.\n",
    "\n",
    "Q4: A) Determine if the model can reproduce realistic data\n",
    "    Generate synthetic data from the fitted model and compare to\n",
    "    observed data. If they don't match, model is missing features.\n",
    "\n",
    "Q5: B) Effective sample size is less than the number of draws\n",
    "    Autocorrelation means consecutive samples are similar. ESS accounts\n",
    "    for this. Don't manually thin (just use all samples but interpret\n",
    "    ESS correctly).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Complete these exercises in the `exercises.ipynb` notebook.\n",
    "\n",
    "### Exercise 1: Implement Adaptive Metropolis-Hastings (Medium)\n",
    "Modify our MH implementation to adaptively tune the proposal standard deviation during burn-in to achieve ~30% acceptance rate.\n",
    "\n",
    "### Exercise 2: Robust Regression with Student-t (Medium)\n",
    "Fit a Bayesian linear model with Student-t likelihood (for robustness to outliers) to oil price data with 5 artificial outliers. Compare to Normal likelihood.\n",
    "\n",
    "### Exercise 3: Hierarchical Model (Hard)\n",
    "Build a hierarchical model for multiple commodities (corn, wheat, soybeans). Allow each to have its own mean but share a common prior. Demonstrate shrinkage.\n",
    "\n",
    "### Exercise 4: Convergence Failure Analysis (Hard)\n",
    "Create a deliberately bad model specification that fails convergence diagnostics. Diagnose the problem using trace plots, R-hat, and ESS. Fix it step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Module Preview\n",
    "\n",
    "In **Module 4: Time Series Fundamentals for Commodities**, we'll learn:\n",
    "- Testing for stationarity (ADF, KPSS tests)\n",
    "- Time series decomposition (trend, seasonality, noise)\n",
    "- ACF/PACF interpretation for model selection\n",
    "- Differencing strategies to achieve stationarity\n",
    "- Commodity-specific time series features\n",
    "- Preparing time series data for Bayesian forecasting\n",
    "\n",
    "---\n",
    "\n",
    "*Module 3 Complete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
